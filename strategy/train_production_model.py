#!/usr/bin/env python3
"""
ç”Ÿäº§çº§AIé‡åŒ–äº¤æ˜“æ¨¡å‹è®­ç»ƒ
ä½¿ç”¨167åªè‚¡ç¥¨çš„5å¹´å†å²æ•°æ®è®­ç»ƒMultiStockTransformerModel
Author: Alvin
"""

import torch
import numpy as np
import pandas as pd
import os
import glob
import logging
from datetime import datetime
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import pickle

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å‹
from models.transformer_model import (
    MultiStockTransformerModel,
    IndustryLeadingFeatureExtractor,
    IndustryLeadingTransformerTrainer
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ProductionModelTrainer:
    """ç”Ÿäº§çº§æ¨¡å‹è®­ç»ƒå™¨"""

    def __init__(self, data_dir: str = "data/training_data"):
        self.data_dir = data_dir
        self.device = self._get_device()

        # æ¨¡å‹é…ç½®ï¼ˆåŸºäºå®é™…æ•°æ®ä¼˜åŒ–ï¼‰
        self.config = {
            'input_dim': 100,     # ä¿æŒ100ç»´ç‰¹å¾
            'time_dim': 8,
            'd_model': 256,       # å¢åŠ æ¨¡å‹å®¹é‡
            'nhead': 16,
            'num_layers': 6,
            'seq_len': 60,        # 60å¤©åºåˆ—é•¿åº¦
            'dropout': 0.1,
            'enable_cross_stock': True
        }

        # åˆå§‹åŒ–ç»„ä»¶
        self.feature_extractor = IndustryLeadingFeatureExtractor()
        self.model = None
        self.trainer = None
        self.scaler = StandardScaler()

    def _get_device(self):
        """è·å–è®¡ç®—è®¾å¤‡"""
        if torch.backends.mps.is_available():
            return 'mps'
        elif torch.cuda.is_available():
            return 'cuda'
        else:
            return 'cpu'

    def load_training_data(self) -> Dict[str, pd.DataFrame]:
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        print("ğŸ“Š åŠ è½½167åªè‚¡ç¥¨çš„è®­ç»ƒæ•°æ®...")

        # è¯»å–æ‰€æœ‰CSVæ–‡ä»¶
        csv_files = glob.glob(os.path.join(self.data_dir, "*.csv"))
        data_dict = {}

        for file_path in csv_files:
            try:
                symbol = os.path.basename(file_path).replace('.csv', '').replace('_', '.')
                df = pd.read_csv(file_path, index_col=0, parse_dates=True)

                if len(df) >= 500:  # è‡³å°‘2å¹´æ•°æ®
                    data_dict[symbol] = df

            except Exception as e:
                logger.warning(f"åŠ è½½ {file_path} å¤±è´¥: {e}")

        print(f"âœ… æˆåŠŸåŠ è½½ {len(data_dict)} åªè‚¡ç¥¨æ•°æ®")

        # é€‰æ‹©æµåŠ¨æ€§æœ€å¥½çš„å‰20åªè‚¡ç¥¨ä½œä¸ºæ ¸å¿ƒè‚¡ç¥¨
        sorted_symbols = sorted(data_dict.items(), key=lambda x: len(x[1]), reverse=True)
        self.core_symbols = [symbol for symbol, _ in sorted_symbols[:20]]

        # æ›´æ–°è‚¡ç¥¨æ•°é‡
        self.config['num_stocks'] = len(self.core_symbols)

        print(f"ğŸ“ˆ æ ¸å¿ƒè‚¡ç¥¨æ± : {self.core_symbols}")

        return data_dict

    def prepare_production_samples(self, data_dict: Dict[str, pd.DataFrame]) -> Tuple:
        """å‡†å¤‡ç”Ÿäº§çº§è®­ç»ƒæ ·æœ¬"""
        print("ğŸ”§ å‡†å¤‡ç”Ÿäº§çº§è®­ç»ƒæ ·æœ¬...")

        all_features = []
        all_time_features = []
        all_targets = []
        all_stock_ids = []

        # åˆ›å»ºè‚¡ç¥¨IDæ˜ å°„
        stock_id_map = {symbol: i for i, symbol in enumerate(self.core_symbols)}

        total_samples = 0

        for symbol in self.core_symbols:
            if symbol not in data_dict:
                continue

            print(f"  å¤„ç† {symbol}...")
            data = data_dict[symbol]
            stock_id = stock_id_map[symbol]

            try:
                # é‡ç½®ç´¢å¼•ï¼Œè·å–æ—¶é—´æˆ³
                data_reset = data.reset_index()
                # å¤„ç†æ—¶åŒºé—®é¢˜
                timestamps_dt = pd.to_datetime(data_reset.iloc[:, 0])
                if timestamps_dt.dt.tz is not None:
                    timestamps_dt = timestamps_dt.dt.tz_localize(None)
                timestamps = timestamps_dt.astype(np.int64) // 10**9

                ohlcv_data = np.column_stack([
                    timestamps,
                    data['Open'].values,
                    data['High'].values,
                    data['Low'].values,
                    data['Close'].values,
                    data['Volume'].values
                ])

                # æå–ç‰¹å¾
                features = self.feature_extractor.extract_features(ohlcv_data)
                time_features = self._extract_time_features(timestamps)

                if features is None or len(features) < self.config['seq_len'] + 10:
                    continue

                # åˆ›å»ºåºåˆ—æ ·æœ¬
                for i in range(self.config['seq_len'], len(features) - 10):
                    # ç‰¹å¾åºåˆ—
                    feature_seq = features[i-self.config['seq_len']:i]
                    time_seq = time_features[i-self.config['seq_len']:i]

                    # åˆ›å»ºå¤šç§æ—¶é—´è·¨åº¦çš„æ ‡ç­¾
                    current_price = ohlcv_data[i, 4]

                    # 1å¤©ã€3å¤©ã€5å¤©ã€10å¤©åçš„ä»·æ ¼
                    future_prices = []
                    valid_sample = True

                    for days in [1, 3, 5, 10]:
                        if i + days < len(features):
                            future_price = ohlcv_data[i + days, 4]
                            future_prices.append(future_price)
                        else:
                            valid_sample = False
                            break

                    if not valid_sample:
                        continue

                    # è®¡ç®—ç»¼åˆæ ‡ç­¾ï¼ˆåŠ æƒå¹³å‡å¤šä¸ªæ—¶é—´è·¨åº¦çš„æ”¶ç›Šç‡ï¼‰
                    returns = [(fp - current_price) / current_price for fp in future_prices]
                    weights = [0.4, 0.3, 0.2, 0.1]  # åé‡çŸ­æœŸé¢„æµ‹
                    weighted_return = sum(r * w for r, w in zip(returns, weights))

                    # æ–¹å‘æ ‡ç­¾
                    if weighted_return > 0.02:
                        direction = 2  # BUY
                    elif weighted_return < -0.02:
                        direction = 0  # SELL
                    else:
                        direction = 1  # HOLD

                    # è®¡ç®—å…¶ä»–æ ‡ç­¾
                    volatility = np.std(returns) if len(returns) > 1 else abs(weighted_return)
                    confidence = min(0.95, abs(weighted_return) * 20 + 0.5)
                    expected_return = weighted_return
                    sharpe_ratio = weighted_return / (volatility + 1e-6)

                    all_features.append(feature_seq)
                    all_time_features.append(time_seq)
                    all_targets.append([direction, volatility, confidence, expected_return, sharpe_ratio])
                    all_stock_ids.append(stock_id)

                    total_samples += 1

            except Exception as e:
                print(f"    âŒ å¤„ç†å¤±è´¥: {e}")
                continue

        print(f"ğŸ”§ ç”Ÿæˆäº† {total_samples} ä¸ªé«˜è´¨é‡è®­ç»ƒæ ·æœ¬")

        # è½¬æ¢ä¸ºtensor
        features_tensor = torch.FloatTensor(np.array(all_features))
        time_tensor = torch.FloatTensor(np.array(all_time_features))
        stock_ids_tensor = torch.LongTensor(all_stock_ids)
        targets_array = np.array(all_targets)

        # æ ‡å‡†åŒ–ç‰¹å¾
        original_shape = features_tensor.shape
        features_flat = features_tensor.reshape(-1, original_shape[-1])
        features_scaled = self.scaler.fit_transform(features_flat.numpy())
        features_tensor = torch.FloatTensor(features_scaled.reshape(original_shape))

        targets_dict = {
            'direction': torch.LongTensor(targets_array[:, 0].astype(int)),
            'volatility': torch.FloatTensor(targets_array[:, 1].reshape(-1, 1)),
            'confidence': torch.FloatTensor(targets_array[:, 2].reshape(-1, 1)),
            'expected_return': torch.FloatTensor(targets_array[:, 3].reshape(-1, 1)),
            'sharpe_ratio': torch.FloatTensor(targets_array[:, 4].reshape(-1, 1))
        }

        return features_tensor, time_tensor, stock_ids_tensor, targets_dict

    def _extract_time_features(self, timestamps: np.ndarray) -> np.ndarray:
        """æå–æ—¶é—´ç‰¹å¾"""
        dt_array = pd.to_datetime(timestamps, unit='s')

        features = np.column_stack([
            dt_array.hour.values,
            dt_array.dayofweek.values,
            dt_array.day.values,
            dt_array.month.values,
            np.sin(2 * np.pi * dt_array.hour / 24),
            np.cos(2 * np.pi * dt_array.hour / 24),
            np.sin(2 * np.pi * dt_array.dayofweek / 7),
            np.cos(2 * np.pi * dt_array.dayofweek / 7)
        ])

        return features.astype(np.float32)

    def train_production_model(self, features: torch.Tensor, time_features: torch.Tensor,
                              stock_ids: torch.Tensor, targets: Dict):
        """è®­ç»ƒç”Ÿäº§çº§æ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒç”Ÿäº§çº§æ¨¡å‹...")

        # åˆ›å»ºæ¨¡å‹
        self.model = MultiStockTransformerModel(
            input_dim=self.config['input_dim'],
            time_dim=self.config['time_dim'],
            d_model=self.config['d_model'],
            nhead=self.config['nhead'],
            num_layers=self.config['num_layers'],
            seq_len=self.config['seq_len'],
            num_stocks=self.config['num_stocks'],
            dropout=self.config['dropout'],
            enable_cross_stock=self.config['enable_cross_stock']
        )

        # åˆ›å»ºè®­ç»ƒå™¨
        self.trainer = IndustryLeadingTransformerTrainer(self.model, self.device)

        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {total_params / 1e6:.2f}M")
        print(f"ğŸ’» è®­ç»ƒè®¾å¤‡: {self.device}")

        # æ•°æ®é›†åˆ†å‰²
        total_samples = len(features)
        train_size = int(0.8 * total_samples)
        val_size = int(0.1 * total_samples)

        # æŒ‰æ—¶é—´é¡ºåºåˆ†å‰²æ•°æ®ï¼ˆæ›´è´´è¿‘å®é™…äº¤æ˜“æƒ…å†µï¼‰
        train_features = features[:train_size]
        train_time = time_features[:train_size]
        train_stock_ids = stock_ids[:train_size]
        train_targets = {k: v[:train_size] for k, v in targets.items()}

        val_features = features[train_size:train_size+val_size]
        val_time = time_features[train_size:train_size+val_size]
        val_stock_ids = stock_ids[train_size:train_size+val_size]
        val_targets = {k: v[train_size:train_size+val_size] for k, v in targets.items()}

        print(f"ğŸ“Š è®­ç»ƒæ ·æœ¬: {len(train_features):,}, éªŒè¯æ ·æœ¬: {len(val_features):,}")

        # è®­ç»ƒå‚æ•°
        batch_size = 64  # å¢åŠ æ‰¹æ¬¡å¤§å°
        num_epochs = 30  # æ›´å¤šè®­ç»ƒè½®æ¬¡

        train_losses = []
        val_accuracies = []
        best_accuracy = 0

        print("ğŸ”„ å¼€å§‹è®­ç»ƒå¾ªç¯...")

        for epoch in range(num_epochs):
            print(f"\nğŸ”„ Epoch {epoch + 1}/{num_epochs}")

            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            epoch_loss = 0
            num_batches = 0

            # éšæœºæ‰“ä¹±è®­ç»ƒæ•°æ®
            indices = torch.randperm(len(train_features))

            for i in range(0, len(train_features), batch_size):
                batch_indices = indices[i:i+batch_size]
                batch_features = train_features[batch_indices]
                batch_time = train_time[batch_indices]
                batch_stock_ids = train_stock_ids[batch_indices]
                batch_targets = {k: v[batch_indices] for k, v in train_targets.items()}

                loss_dict = self.trainer.train_step(
                    batch_features, batch_time, batch_targets, batch_stock_ids
                )

                epoch_loss += loss_dict['total_loss']
                num_batches += 1

                if num_batches % 50 == 0:
                    print(f"  Batch {num_batches}: Loss = {loss_dict['total_loss']:.4f}")

            avg_loss = epoch_loss / num_batches
            train_losses.append(avg_loss)

            # éªŒè¯é˜¶æ®µ
            metrics = self.trainer.evaluate(
                val_features, val_time, val_targets, val_stock_ids
            )
            val_accuracies.append(metrics['direction_accuracy'])

            print(f"  è®­ç»ƒæŸå¤±: {avg_loss:.4f}")
            print(f"  éªŒè¯å‡†ç¡®ç‡: {metrics['direction_accuracy']:.4f}")
            print(f"  éªŒè¯æŒ‡æ ‡: {metrics}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if metrics['direction_accuracy'] > best_accuracy:
                best_accuracy = metrics['direction_accuracy']
                model_path = 'models/best_production_model.pth'
                self.trainer.save_model(model_path)
                print(f"  ğŸ¯ ä¿å­˜æœ€ä½³æ¨¡å‹ (å‡†ç¡®ç‡: {best_accuracy:.4f})")

            # å­¦ä¹ ç‡è¡°å‡
            if epoch > 0 and epoch % 10 == 0:
                for param_group in self.trainer.optimizer.param_groups:
                    param_group['lr'] *= 0.8
                print(f"  ğŸ“‰ å­¦ä¹ ç‡è¡°å‡è‡³: {param_group['lr']:.6f}")

        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self._plot_training_curves(train_losses, val_accuracies)

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œé…ç½®
        final_model_path = 'models/final_production_model.pth'
        self.trainer.save_model(final_model_path)

        # ä¿å­˜scaler
        with open('models/feature_scaler.pkl', 'wb') as f:
            pickle.dump(self.scaler, f)

        # ä¿å­˜é…ç½®
        config_save = {
            'model_config': self.config,
            'core_symbols': self.core_symbols,
            'stock_id_map': {symbol: i for i, symbol in enumerate(self.core_symbols)},
            'best_accuracy': best_accuracy,
            'training_samples': len(train_features),
            'training_date': datetime.now().isoformat()
        }

        with open('models/production_config.pkl', 'wb') as f:
            pickle.dump(config_save, f)

        print(f"âœ… ç”Ÿäº§æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ¯ æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_accuracy:.4f}")
        print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜åˆ°: {final_model_path}")

    def _plot_training_curves(self, losses: List[float], accuracies: List[float]):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        plt.figure(figsize=(15, 5))

        plt.subplot(1, 3, 1)
        plt.plot(losses)
        plt.title('Training Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.grid(True)

        plt.subplot(1, 3, 2)
        plt.plot(accuracies)
        plt.title('Validation Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.grid(True)

        plt.subplot(1, 3, 3)
        if len(losses) > 10:
            smooth_losses = np.convolve(losses, np.ones(5)/5, mode='valid')
            plt.plot(smooth_losses, label='Smoothed Loss')
        plt.plot(accuracies, label='Accuracy', color='orange')
        plt.title('Training Progress')
        plt.xlabel('Epoch')
        plt.legend()
        plt.grid(True)

        plt.tight_layout()
        plt.savefig('production_training_curves.png', dpi=300, bbox_inches='tight')
        print("ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: production_training_curves.png")

    def run_production_training(self):
        """è¿è¡Œå®Œæ•´çš„ç”Ÿäº§çº§è®­ç»ƒ"""
        print("=" * 80)
        print("ğŸš€ ç”Ÿäº§çº§AIé‡åŒ–äº¤æ˜“æ¨¡å‹è®­ç»ƒ")
        print("=" * 80)

        try:
            # 1. åŠ è½½æ•°æ®
            data_dict = self.load_training_data()
            if len(data_dict) < 10:
                print("âŒ æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒ")
                return

            # 2. å‡†å¤‡è®­ç»ƒæ ·æœ¬
            features, time_features, stock_ids, targets = self.prepare_production_samples(data_dict)

            if len(features) < 10000:
                print("âŒ è®­ç»ƒæ ·æœ¬ä¸è¶³")
                return

            # 3. è®­ç»ƒæ¨¡å‹
            self.train_production_model(features, time_features, stock_ids, targets)

            print("\nâœ… ç”Ÿäº§çº§æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
            print("ğŸ’¡ æ¨¡å‹å·²å¯ç”¨äºå®é™…äº¤æ˜“ä¿¡å·ç”Ÿæˆ")

        except Exception as e:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    trainer = ProductionModelTrainer()
    trainer.run_production_training()