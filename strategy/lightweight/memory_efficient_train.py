#!/usr/bin/env python3
"""
å†…å­˜ä¼˜åŒ–è®­ç»ƒè„šæœ¬ - è§£å†³MPSå†…å­˜ä¸è¶³é—®é¢˜
ä½¿ç”¨æ›´å°çš„æ¨¡åž‹å’Œæ‰¹æ¬¡å¤§å°ï¼ŒCPUéªŒè¯
Author: Alvin
"""

import torch
import numpy as np
import pandas as pd
import os
import logging
from datetime import datetime
import pickle
from sklearn.preprocessing import StandardScaler

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def calculate_basic_features(df):
    """è®¡ç®—åŸºç¡€æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾"""
    features = pd.DataFrame(index=df.index)

    # ä»·æ ¼ç‰¹å¾
    features['open'] = df['Open']
    features['high'] = df['High']
    features['low'] = df['Low']
    features['close'] = df['Close']
    features['volume'] = df['Volume']

    # æ”¶ç›ŠçŽ‡
    features['returns'] = df['Close'].pct_change()
    features['log_returns'] = np.log(df['Close'] / df['Close'].shift(1))

    # ç§»åŠ¨å¹³å‡
    for window in [5, 10, 20]:  # å‡å°‘ç‰¹å¾æ•°é‡
        features[f'ma_{window}'] = df['Close'].rolling(window).mean()
        features[f'price_to_ma_{window}'] = df['Close'] / features[f'ma_{window}']

    # RSI
    delta = df['Close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    features['rsi'] = 100 - (100 / (1 + rs))

    # å¸ƒæž—å¸¦
    ma20 = df['Close'].rolling(20).mean()
    std20 = df['Close'].rolling(20).std()
    features['bb_upper'] = ma20 + (std20 * 2)
    features['bb_lower'] = ma20 - (std20 * 2)
    features['bb_position'] = (df['Close'] - features['bb_lower']) / (features['bb_upper'] - features['bb_lower'])

    # MACD
    ema12 = df['Close'].ewm(span=12).mean()
    ema26 = df['Close'].ewm(span=26).mean()
    features['macd'] = ema12 - ema26

    # æ³¢åŠ¨çŽ‡
    features['volatility'] = features['returns'].rolling(20).std()

    # æˆäº¤é‡æŒ‡æ ‡
    features['volume_ma'] = df['Volume'].rolling(20).mean()
    features['volume_ratio'] = df['Volume'] / features['volume_ma']

    # ç¼ºå¤±å€¼å¤„ç†
    features = features.fillna(method='ffill').fillna(0)

    return features

def memory_efficient_transformer():
    """åˆ›å»ºå†…å­˜ä¼˜åŒ–çš„å°åž‹Transformeræ¨¡åž‹"""

    class TinyTransformer(torch.nn.Module):
        def __init__(self, input_dim, d_model=64, nhead=4, num_layers=2, num_classes=3):  # æ›´å°çš„æ¨¡åž‹
            super().__init__()

            self.input_projection = torch.nn.Linear(input_dim, d_model)
            self.pos_encoding = torch.nn.Parameter(torch.randn(100, d_model) * 0.1)  # æ›´çŸ­çš„ä½ç½®ç¼–ç 

            encoder_layer = torch.nn.TransformerEncoderLayer(
                d_model=d_model,
                nhead=nhead,
                dim_feedforward=128,  # æ›´å°çš„å‰é¦ˆç½‘ç»œ
                dropout=0.1,
                batch_first=True
            )
            self.transformer = torch.nn.TransformerEncoder(encoder_layer, num_layers)

            self.classifier = torch.nn.Sequential(
                torch.nn.Linear(d_model, 32),  # æ›´å°çš„åˆ†ç±»å™¨
                torch.nn.ReLU(),
                torch.nn.Dropout(0.1),
                torch.nn.Linear(32, num_classes)
            )

        def forward(self, x):
            # x: (batch, seq_len, features)
            seq_len = x.size(1)

            # æŠ•å½±åˆ°æ¨¡åž‹ç»´åº¦
            x = self.input_projection(x)

            # æ·»åŠ ä½ç½®ç¼–ç ï¼ˆæˆªæ–­åˆ°å®žé™…åºåˆ—é•¿åº¦ï¼‰
            pos_len = min(seq_len, self.pos_encoding.size(0))
            x[:, :pos_len, :] = x[:, :pos_len, :] + self.pos_encoding[:pos_len].unsqueeze(0)

            # Transformerç¼–ç 
            x = self.transformer(x)

            # ä½¿ç”¨æœ€åŽä¸€ä¸ªæ—¶é—´æ­¥è¿›è¡Œåˆ†ç±»
            x = x[:, -1, :]

            # åˆ†ç±»
            output = self.classifier(x)

            return output

    return TinyTransformer

def memory_efficient_train():
    """å†…å­˜ä¼˜åŒ–è®­ç»ƒå‡½æ•°"""
    logger.info("ðŸš€ å¼€å§‹å†…å­˜ä¼˜åŒ–è®­ç»ƒ")

    # èŽ·å–æ•°æ®æ–‡ä»¶
    data_dir = "data/training_data"
    all_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]

    # åªä½¿ç”¨å‰20ä¸ªæ–‡ä»¶è¿›è¡Œå†…å­˜å‹å¥½è®­ç»ƒ
    selected_files = all_files[:20]
    logger.info(f"ðŸ“Š é€‰æ‹© {len(selected_files)} åªè‚¡ç¥¨è¿›è¡Œè®­ç»ƒ")

    # æ”¶é›†æ‰€æœ‰æ•°æ®
    all_features = []
    all_targets = []

    logger.info("ðŸ“¦ å¼€å§‹æ•°æ®åŠ è½½å’Œç‰¹å¾è®¡ç®—...")

    for i, file in enumerate(selected_files):
        try:
            file_path = os.path.join(data_dir, file)
            df = pd.read_csv(file_path, index_col=0, parse_dates=True)

            if len(df) < 200:  # è·³è¿‡æ•°æ®å¤ªå°‘çš„è‚¡ç¥¨
                continue

            # è®¡ç®—ç‰¹å¾
            features = calculate_basic_features(df)

            if len(features) > 70:
                # åˆ›å»ºåºåˆ—æ•°æ® - ä½¿ç”¨æ›´çŸ­çš„åºåˆ—
                seq_len = 30  # å‡å°‘åˆ°30ä¸ªæ—¶é—´æ­¥
                for j in range(seq_len, min(len(features) - 1, seq_len + 500)):  # é™åˆ¶æ ·æœ¬æ•°é‡
                    # ç‰¹å¾åºåˆ— (å–æ•°å€¼åˆ—)
                    feature_seq = features.iloc[j-seq_len:j].select_dtypes(include=[np.number]).values

                    # ç›®æ ‡ï¼ˆä¸‹ä¸€å¤©çš„æ”¶ç›ŠçŽ‡ï¼‰
                    next_return = df['Close'].iloc[j+1] / df['Close'].iloc[j] - 1
                    if next_return > 0.02:
                        target = 0  # BUY
                    elif next_return < -0.02:
                        target = 2  # SELL
                    else:
                        target = 1  # HOLD

                    all_features.append(feature_seq)
                    all_targets.append(target)

            logger.info(f"âœ… å·²å¤„ç† {i+1}/{len(selected_files)} åªè‚¡ç¥¨ï¼Œæ ·æœ¬æ•°: {len(all_features)}")

        except Exception as e:
            logger.warning(f"âŒ å¤„ç† {file} å¤±è´¥: {e}")
            continue

    logger.info(f"ðŸ“Š æ•°æ®æ”¶é›†å®Œæˆ: {len(all_features)} ä¸ªæ ·æœ¬")

    if len(all_features) < 100:
        logger.error("âŒ è®­ç»ƒæ ·æœ¬å¤ªå°‘ï¼Œæ— æ³•è®­ç»ƒ")
        return

    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    X = np.array(all_features, dtype=np.float32)
    y = np.array(all_targets, dtype=np.int64)

    logger.info(f"ðŸ“ ç‰¹å¾å½¢çŠ¶: {X.shape}")
    logger.info(f"ðŸ“ ç›®æ ‡å½¢çŠ¶: {y.shape}")

    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)

    # åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_size = int(0.8 * len(X_scaled))
    X_train, X_test = X_scaled[:train_size], X_scaled[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]

    logger.info(f"ðŸ“Š è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
    logger.info(f"ðŸ“Š æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")

    # èŽ·å–è®¾å¤‡
    device = 'mps' if torch.backends.mps.is_available() else 'cpu'
    logger.info(f"ðŸ’» ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºæ¨¡åž‹
    input_dim = X.shape[-1]
    TinyTransformerClass = memory_efficient_transformer()
    model = TinyTransformerClass(input_dim=input_dim).to(device)

    logger.info("ðŸ¤– å†…å­˜ä¼˜åŒ–æ¨¡åž‹åˆ›å»ºæˆåŠŸ")

    # è®­ç»ƒè®¾ç½®
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # è½¬æ¢ä¸ºtensor
    X_train_tensor = torch.FloatTensor(X_train).to(device)
    y_train_tensor = torch.LongTensor(y_train).to(device)

    logger.info("ðŸš€ å¼€å§‹è®­ç»ƒ...")

    # è®­ç»ƒå¾ªçŽ¯
    batch_size = 16  # æ›´å°çš„æ‰¹æ¬¡å¤§å°
    num_epochs = 10

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        num_batches = 0

        # æ‰¹æ¬¡è®­ç»ƒ
        for i in range(0, len(X_train_tensor), batch_size):
            batch_x = X_train_tensor[i:i+batch_size]
            batch_y = y_train_tensor[i:i+batch_size]

            optimizer.zero_grad()
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            num_batches += 1

            # æ¸…ç†GPUå†…å­˜
            if device == 'mps':
                torch.mps.empty_cache()

        avg_loss = total_loss / num_batches
        logger.info(f"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}")

        # æ¯3ä¸ªepochéªŒè¯ä¸€æ¬¡ï¼Œä½¿ç”¨CPUé¿å…å†…å­˜é—®é¢˜
        if (epoch + 1) % 3 == 0:
            model.eval()
            model_cpu = model.cpu()  # ç§»åˆ°CPUè¿›è¡ŒéªŒè¯

            with torch.no_grad():
                X_test_cpu = torch.FloatTensor(X_test)
                y_test_cpu = torch.LongTensor(y_test)

                # åˆ†æ‰¹éªŒè¯
                correct = 0
                total = 0
                for i in range(0, len(X_test_cpu), batch_size):
                    batch_x = X_test_cpu[i:i+batch_size]
                    batch_y = y_test_cpu[i:i+batch_size]

                    outputs = model_cpu(batch_x)
                    predictions = torch.argmax(outputs, dim=1)
                    correct += (predictions == batch_y).sum().item()
                    total += batch_y.size(0)

                accuracy = correct / total
                logger.info(f"éªŒè¯å‡†ç¡®çŽ‡: {accuracy:.4f}")

            # ç§»å›žGPUç»§ç»­è®­ç»ƒ
            model = model_cpu.to(device)

    logger.info("âœ… è®­ç»ƒå®Œæˆ")

    # æœ€ç»ˆéªŒè¯å¹¶ä¿å­˜æ¨¡åž‹
    model.eval()
    model_cpu = model.cpu()

    # ä¿å­˜æ¨¡åž‹
    model_save_path = "tiny_transformer_model.pth"
    torch.save(model_cpu.state_dict(), model_save_path)
    logger.info(f"ðŸ’¾ æ¨¡åž‹å·²ä¿å­˜: {model_save_path}")

    # ä¿å­˜ç¼©æ”¾å™¨
    scaler_save_path = "tiny_scaler.pkl"
    with open(scaler_save_path, 'wb') as f:
        pickle.dump(scaler, f)
    logger.info(f"ðŸ’¾ ç¼©æ”¾å™¨å·²ä¿å­˜: {scaler_save_path}")

    # æœ€ç»ˆéªŒè¯
    with torch.no_grad():
        X_test_cpu = torch.FloatTensor(X_test)
        y_test_cpu = torch.LongTensor(y_test)

        # åˆ†æ‰¹éªŒè¯
        correct = 0
        total = 0
        for i in range(0, len(X_test_cpu), batch_size):
            batch_x = X_test_cpu[i:i+batch_size]
            batch_y = y_test_cpu[i:i+batch_size]

            outputs = model_cpu(batch_x)
            predictions = torch.argmax(outputs, dim=1)
            correct += (predictions == batch_y).sum().item()
            total += batch_y.size(0)

        final_accuracy = correct / total
        logger.info(f"ðŸŽ¯ æœ€ç»ˆå‡†ç¡®çŽ‡: {final_accuracy:.4f}")

    # ä¿å­˜æ¨¡åž‹ç»“æž„ä¿¡æ¯
    model_info = {
        'input_dim': input_dim,
        'model_class': 'TinyTransformer',
        'accuracy': final_accuracy,
        'num_samples': len(X),
        'seq_len': 30,
        'd_model': 64,
        'feature_names': list(range(input_dim))
    }

    with open('tiny_model_info.pkl', 'wb') as f:
        pickle.dump(model_info, f)

    logger.info("ðŸŽ‰ å†…å­˜ä¼˜åŒ–è®­ç»ƒå®Œæˆ!")
    return model_save_path, scaler_save_path

if __name__ == "__main__":
    memory_efficient_train()