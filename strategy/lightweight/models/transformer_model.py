"""
ä¸šç•Œæœ€ä¼˜AIé‡åŒ–äº¤æ˜“æ¨¡å‹ - è½»é‡çº§Transformeræ¶æ„
Author: Alvin
ä¸“ä¸ºMac Miniä¼˜åŒ–ï¼Œå®ç°ä¸šç•Œä¸€æµçš„äº¤æ˜“ä¿¡å·ç”Ÿæˆ
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import math
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

class Time2Vec(nn.Module):
    """
    Time2Vecç¼–ç æ¨¡å— - ä¸šç•Œæœ€ä¼˜æ—¶é—´ç¼–ç æ–¹æ¡ˆ
    Paper: "Time2Vec: Learning a Vector Representation of Time"
    ä¸“ä¸ºé‡‘èæ—¶é—´åºåˆ—è®¾è®¡ï¼Œä¼˜äºä¼ ç»Ÿä½ç½®ç¼–ç 
    """

    def __init__(self, input_dim: int, embed_dim: int):
        super().__init__()
        self.embed_dim = embed_dim

        # çº¿æ€§å±‚ç”¨äºéå‘¨æœŸæ€§ç‰¹å¾
        self.linear_layer = nn.Linear(input_dim, 1)

        # å‘¨æœŸæ€§å±‚æ•°ï¼ˆembed_dim - 1ï¼Œå› ä¸ºçº¿æ€§å±‚å ç”¨1ç»´ï¼‰
        self.periodic_layers = nn.ModuleList([
            nn.Linear(input_dim, 1) for _ in range(embed_dim - 1)
        ])

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: (batch_size, seq_len, input_dim) - æ—¶é—´ç‰¹å¾
        è¿”å›: (batch_size, seq_len, embed_dim)
        """
        batch_size, seq_len, _ = x.shape

        # éå‘¨æœŸæ€§éƒ¨åˆ†
        linear_out = self.linear_layer(x)  # (batch, seq, 1)

        # å‘¨æœŸæ€§éƒ¨åˆ†
        periodic_outs = []
        for layer in self.periodic_layers:
            periodic_out = torch.sin(layer(x))  # ä½¿ç”¨sinæ¿€æ´»
            periodic_outs.append(periodic_out)

        # æ‹¼æ¥æ‰€æœ‰ç»´åº¦
        periodic_tensor = torch.cat(periodic_outs, dim=-1)  # (batch, seq, embed_dim-1)

        # ç»„åˆçº¿æ€§å’Œå‘¨æœŸæ€§ç‰¹å¾
        time2vec_out = torch.cat([linear_out, periodic_tensor], dim=-1)  # (batch, seq, embed_dim)

        return time2vec_out

class AdvancedPositionalEncoding(nn.Module):
    """
    å¢å¼ºä½ç½®ç¼–ç æ¨¡å—
    ç»“åˆä¼ ç»Ÿä½ç½®ç¼–ç å’Œå¯å­¦ä¹ ä½ç½®åµŒå…¥
    """

    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)

        # ä¼ ç»Ÿæ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç 
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                           (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

        # å¯å­¦ä¹ ä½ç½®åµŒå…¥
        self.learned_pe = nn.Parameter(torch.randn(max_len, d_model) * 0.1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        seq_len = x.size(1)

        # ç»„åˆå›ºå®šå’Œå¯å­¦ä¹ ä½ç½®ç¼–ç 
        pos_encoding = self.pe[:seq_len, :] + self.learned_pe[:seq_len, :]

        return self.dropout(x + pos_encoding.unsqueeze(0))

class MultiStockTransformerModel(nn.Module):
    """
    ä¸šç•Œæœ€ä¼˜å¤šè‚¡ç¥¨ååŒé¢„æµ‹Transformeræ¨¡å‹
    åŸºäºæœ€æ–°ç ”ç©¶ï¼šTime2Vec + Multi-feature + Cross-stock attention
    æ”¯æŒå¤šè‚¡ç¥¨ååŒé¢„æµ‹ã€å¢å¼ºç‰¹å¾å·¥ç¨‹ã€å®Œæ•´å¯è§£é‡Šæ€§
    """

    def __init__(self,
                 input_dim: int = 50,          # åŸºç¡€ç‰¹å¾ç»´åº¦
                 time_dim: int = 8,            # æ—¶é—´ç‰¹å¾ç»´åº¦
                 d_model: int = 256,           # å¢å¼ºåµŒå…¥ç»´åº¦
                 nhead: int = 16,              # å¢å¼ºæ³¨æ„åŠ›å¤´æ•°
                 num_layers: int = 6,          # å¢å¼ºå±‚æ•°
                 seq_len: int = 60,            # åºåˆ—é•¿åº¦
                 num_stocks: int = 10,         # æ”¯æŒå¤šè‚¡ç¥¨ååŒ
                 dropout: float = 0.1,
                 enable_cross_stock: bool = True):
        super().__init__()

        self.d_model = d_model
        self.seq_len = seq_len
        self.num_stocks = num_stocks
        self.enable_cross_stock = enable_cross_stock

        # ä¸šç•Œæœ€ä¼˜Time2Vecæ—¶é—´ç¼–ç 
        self.time2vec = Time2Vec(time_dim, d_model // 4)

        # ç‰¹å¾æŠ•å½±å±‚ - å¢å¼ºå®¹é‡
        self.feature_projection = nn.Linear(input_dim, d_model * 3 // 4)

        # ç»¼åˆç‰¹å¾èåˆ
        self.feature_fusion = nn.Linear(d_model, d_model)

        # å¢å¼ºä½ç½®ç¼–ç 
        self.pos_encoding = AdvancedPositionalEncoding(d_model, seq_len, dropout)

        # è‚¡ç¥¨åµŒå…¥ (ç”¨äºå¤šè‚¡ç¥¨ååŒ)
        if enable_cross_stock:
            self.stock_embedding = nn.Embedding(num_stocks, d_model)

        # ä¸»è¦Transformerç¼–ç å™¨ - å¢å¼ºæ¶æ„
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,  # ä¸šç•Œæ ‡å‡†4å€æ”¾å¤§
            dropout=dropout,
            batch_first=True,
            activation='gelu',
            norm_first=True  # Pre-LayerNormæ¶æ„ï¼Œæ›´ç¨³å®š
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)

        # è·¨è‚¡ç¥¨æ³¨æ„åŠ›æ¨¡å—
        if enable_cross_stock:
            self.cross_stock_attention = nn.MultiheadAttention(
                embed_dim=d_model,
                num_heads=nhead,
                dropout=dropout,
                batch_first=True
            )
            self.cross_stock_norm = nn.LayerNorm(d_model)

        # ä¸šç•Œæœ€ä¼˜å¤šä»»åŠ¡è¾“å‡ºå¤´ - å¢å¼ºç½‘ç»œ
        hidden_dim = d_model // 2

        self.direction_head = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(d_model, hidden_dim),
            nn.GELU(),
            nn.LayerNorm(hidden_dim),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.GELU(),
            nn.Linear(hidden_dim // 2, 3)  # BUY/HOLD/SELL
        )

        self.volatility_head = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(d_model, hidden_dim),
            nn.GELU(),
            nn.LayerNorm(hidden_dim),
            nn.Linear(hidden_dim, 1)  # æ³¢åŠ¨ç‡é¢„æµ‹
        )

        self.confidence_head = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(d_model, hidden_dim),
            nn.GELU(),
            nn.LayerNorm(hidden_dim),
            nn.Linear(hidden_dim, 1),  # ç½®ä¿¡åº¦é¢„æµ‹
            nn.Sigmoid()  # ç¡®ä¿0-1èŒƒå›´
        )

        self.return_head = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(d_model, hidden_dim),
            nn.GELU(),
            nn.LayerNorm(hidden_dim),
            nn.Linear(hidden_dim, 1)  # æ”¶ç›Šç‡é¢„æµ‹
        )

        # æ–°å¢ï¼šé£é™©è°ƒæ•´æ”¶ç›Šé¢„æµ‹
        self.sharpe_head = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(d_model, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, 1)  # å¤æ™®æ¯”ç‡é¢„æµ‹
        )

        # åˆå§‹åŒ–æƒé‡ - ä¸šç•Œæœ€ä½³å®è·µ
        self._init_weights()

        # å­˜å‚¨æ³¨æ„åŠ›æƒé‡ç”¨äºå¯è§£é‡Šæ€§
        self.attention_weights = None

    def _init_weights(self):
        """ä¸šç•Œæœ€ä½³æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                # Xavieråˆå§‹åŒ–é€‚åˆGELUæ¿€æ´»
                nn.init.xavier_uniform_(m.weight, gain=1.0)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.weight, 1.0)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Embedding):
                nn.init.normal_(m.weight, mean=0, std=0.1)

    def forward(self,
                features: torch.Tensor,
                time_features: torch.Tensor,
                stock_ids: Optional[torch.Tensor] = None,
                mask: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:
        """
        ä¸šç•Œæœ€ä¼˜å‰å‘ä¼ æ’­
        features: (batch_size, seq_len, input_dim) - ä¸»è¦ç‰¹å¾
        time_features: (batch_size, seq_len, time_dim) - æ—¶é—´ç‰¹å¾
        stock_ids: (batch_size,) - è‚¡ç¥¨IDç”¨äºååŒé¢„æµ‹
        mask: (batch_size, seq_len) - æ³¨æ„åŠ›æ©ç 
        """
        batch_size, seq_len, _ = features.shape

        # Time2Vecæ—¶é—´ç¼–ç 
        time_encoded = self.time2vec(time_features)  # (batch, seq, d_model//4)

        # ç‰¹å¾æŠ•å½±
        feature_projected = self.feature_projection(features)  # (batch, seq, 3*d_model//4)

        # ç‰¹å¾èåˆ
        combined_features = torch.cat([feature_projected, time_encoded], dim=-1)
        fused_features = self.feature_fusion(combined_features)  # (batch, seq, d_model)

        # è‚¡ç¥¨åµŒå…¥ (å¤šè‚¡ç¥¨ååŒ)
        if self.enable_cross_stock and stock_ids is not None:
            stock_embeds = self.stock_embedding(stock_ids)  # (batch, d_model)
            stock_embeds = stock_embeds.unsqueeze(1).expand(-1, seq_len, -1)
            fused_features = fused_features + stock_embeds

        # ä½ç½®ç¼–ç 
        encoded_features = self.pos_encoding(fused_features)

        # ä¸»è¦Transformerç¼–ç 
        transformer_out = self.transformer(encoded_features, src_key_padding_mask=mask)

        # è·¨è‚¡ç¥¨æ³¨æ„åŠ› (å¤šè‚¡ç¥¨ååŒé¢„æµ‹)
        if self.enable_cross_stock:
            cross_attended, self.attention_weights = self.cross_stock_attention(
                transformer_out, transformer_out, transformer_out, key_padding_mask=mask
            )
            transformer_out = self.cross_stock_norm(transformer_out + cross_attended)

        # åºåˆ—æ± åŒ– - ä½¿ç”¨æ³¨æ„åŠ›æ± åŒ–æ›¿ä»£ç®€å•çš„æœ€åä½ç½®
        if mask is not None:
            # è®¡ç®—æœ‰æ•ˆåºåˆ—é•¿åº¦
            seq_lengths = (~mask).sum(dim=1, keepdim=True).float()
            # æ©ç æ± åŒ–
            masked_out = transformer_out.masked_fill(mask.unsqueeze(-1), 0)
            pooled_output = masked_out.sum(dim=1) / seq_lengths.clamp(min=1)
        else:
            # å¹³å‡æ± åŒ–
            pooled_output = transformer_out.mean(dim=1)

        # å¤šä»»åŠ¡è¾“å‡º
        outputs = {
            'direction': self.direction_head(pooled_output),
            'volatility': torch.abs(self.volatility_head(pooled_output)),  # ç¡®ä¿éè´Ÿ
            'confidence': self.confidence_head(pooled_output),
            'expected_return': self.return_head(pooled_output),
            'sharpe_ratio': self.sharpe_head(pooled_output)  # æ–°å¢é£é™©è°ƒæ•´æ”¶ç›Š
        }

        return outputs

    def get_attention_weights(self) -> Optional[torch.Tensor]:
        """è·å–æœ€æ–°çš„æ³¨æ„åŠ›æƒé‡ç”¨äºå¯è§£é‡Šæ€§åˆ†æ"""
        return self.attention_weights

    def get_feature_importance(self,
                             features: torch.Tensor,
                             time_features: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—ç‰¹å¾é‡è¦æ€§ç”¨äºå¯è§£é‡Šæ€§
        ä½¿ç”¨æ¢¯åº¦æ–¹æ³•è®¡ç®—æ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§
        """
        features.requires_grad_(True)
        time_features.requires_grad_(True)

        outputs = self.forward(features, time_features)

        # å¯¹confidenceè¾“å‡ºè®¡ç®—æ¢¯åº¦
        confidence_score = outputs['confidence'].sum()
        confidence_score.backward(retain_graph=True)

        # è¿”å›ç‰¹å¾é‡è¦æ€§
        feature_importance = torch.abs(features.grad).mean(dim=(0, 1))
        time_importance = torch.abs(time_features.grad).mean(dim=(0, 1))

        return feature_importance, time_importance

class IndustryLeadingFeatureExtractor:
    """
    ä¸šç•Œæœ€ä¼˜ç‰¹å¾å·¥ç¨‹æ¨¡å—
    åŸºäºæœ€æ–°é‡‘èå·¥ç¨‹ç ”ç©¶ï¼Œå®ç°ä¸šç•Œé¢†å…ˆçš„ç‰¹å¾æå–
    åŒ…å«ï¼šæŠ€æœ¯æŒ‡æ ‡ã€å› å­åº“ã€å®è§‚ç‰¹å¾ã€æƒ…ç»ªæŒ‡æ ‡ã€é«˜é¢‘å¾®è§‚ç»“æ„ç‰¹å¾
    """

    def __init__(self):
        self.lookback_periods = [3, 5, 10, 20, 50, 100, 200]  # æ‰©å±•æ—¶é—´çª—å£
        self.volume_periods = [5, 10, 20]
        self.momentum_periods = [5, 10, 20, 60]  # åŠ¨é‡å‘¨æœŸ

    def extract_comprehensive_features(self, ohlcv_data: np.ndarray,
                                     market_data: Optional[Dict] = None) -> Dict[str, np.ndarray]:
        """
        ä¸šç•Œæœ€å…¨é¢çš„ç‰¹å¾æå–
        ohlcv_data: [timestamp, open, high, low, close, volume]
        market_data: å¸‚åœºå®è§‚æ•°æ® (å¯é€‰)
        """
        if len(ohlcv_data) < 200:  # ç¡®ä¿è¶³å¤Ÿçš„å†å²æ•°æ®
            raise ValueError("éœ€è¦è‡³å°‘200ä¸ªæ•°æ®ç‚¹è¿›è¡Œç‰¹å¾æå–")

        close = ohlcv_data[:, 4]
        high = ohlcv_data[:, 2]
        low = ohlcv_data[:, 3]
        open_price = ohlcv_data[:, 1]
        volume = ohlcv_data[:, 5]
        timestamp = ohlcv_data[:, 0]

        features = {}

        # 1. åŸºç¡€ä»·æ ¼ç‰¹å¾ (å¢å¼ºç‰ˆ)
        features.update(self._extract_enhanced_price_features(open_price, high, low, close, volume))

        # 2. é«˜çº§æŠ€æœ¯æŒ‡æ ‡ (ä¸šç•Œæ ‡å‡†)
        features.update(self._calculate_advanced_technical_indicators(ohlcv_data))

        # 3. é‡ä»·å…³ç³»ç‰¹å¾
        features.update(self._extract_volume_price_features(close, volume))

        # 4. åŠ¨é‡å’Œè¶‹åŠ¿ç‰¹å¾
        features.update(self._extract_momentum_features(close, high, low))

        # 5. æ³¢åŠ¨ç‡å»ºæ¨¡ç‰¹å¾
        features.update(self._extract_volatility_modeling_features(close))

        # 6. å¸‚åœºå¾®è§‚ç»“æ„ç‰¹å¾ (é«˜é¢‘)
        features.update(self._extract_microstructure_features(ohlcv_data))

        # 7. æ—¶é—´åºåˆ—ç»Ÿè®¡ç‰¹å¾
        features.update(self._extract_statistical_features(close, volume))

        # 8. è·¨å‘¨æœŸç›¸å¯¹å¼ºå¼±ç‰¹å¾
        features.update(self._extract_relative_strength_features(close))

        # 9. é£é™©åº¦é‡ç‰¹å¾
        features.update(self._extract_risk_features(close))

        # 10. æ—¶é—´ç‰¹å¾ (ç”¨äºTime2Vec)
        features.update(self._extract_time_features(timestamp))

        return features

    def _extract_enhanced_price_features(self, open_price, high, low, close, volume):
        """å¢å¼ºä»·æ ¼ç‰¹å¾"""
        features = {}

        # å¤šæ—¶é—´æ¡†æ¶æ”¶ç›Šç‡
        for period in self.lookback_periods:
            if len(close) > period:
                # å¯¹æ•°æ”¶ç›Šç‡
                features[f'log_return_{period}'] = np.log(close[period:] / close[:-period])
                # ç´¯ç§¯æ”¶ç›Šç‡
                features[f'cum_return_{period}'] = (close[period:] / close[:-period]) - 1
                # é«˜ä½ç‚¹æ”¶ç›Šç‡
                features[f'high_return_{period}'] = np.log(high[period:] / close[:-period])
                features[f'low_return_{period}'] = np.log(low[period:] / close[:-period])

        # ä»·æ ¼ä½ç½®ç‰¹å¾
        for period in [10, 20, 50]:
            if len(close) > period:
                rolling_high = np.array([np.max(high[max(0, i-period+1):i+1])
                                       for i in range(len(high))])
                rolling_low = np.array([np.min(low[max(0, i-period+1):i+1])
                                      for i in range(len(low))])
                features[f'price_position_{period}'] = (close - rolling_low) / (rolling_high - rolling_low + 1e-8)

        # ä»·å·®ç‰¹å¾
        features['high_low_ratio'] = (high - low) / close
        features['open_close_ratio'] = (close - open_price) / open_price
        features['close_high_ratio'] = close / high
        features['close_low_ratio'] = close / low

        return features

    def _calculate_advanced_technical_indicators(self, ohlcv_data):
        """ä¸šç•Œæ ‡å‡†æŠ€æœ¯æŒ‡æ ‡"""
        close = ohlcv_data[:, 4]
        high = ohlcv_data[:, 2]
        low = ohlcv_data[:, 3]
        volume = ohlcv_data[:, 5]

        features = {}

        # RSI (å¤šå‘¨æœŸ)
        for period in [9, 14, 21]:
            features[f'rsi_{period}'] = self._calculate_rsi(close, period)

        # MACD (å¤šå‚æ•°ç»„åˆ)
        macd_configs = [(12, 26, 9), (5, 35, 5), (19, 39, 9)]
        for i, (fast, slow, signal) in enumerate(macd_configs):
            macd, macd_signal, macd_hist = self._calculate_macd(close, fast, slow, signal)
            features[f'macd_{i}'] = macd
            features[f'macd_signal_{i}'] = macd_signal
            features[f'macd_histogram_{i}'] = macd_hist

        # å¸ƒæ—å¸¦ (å¤šæ ‡å‡†å·®)
        for period, std_factor in [(20, 2), (20, 1.5), (10, 2)]:
            bb_upper, bb_middle, bb_lower = self._calculate_bollinger_bands(close, period, std_factor)
            idx = f"{period}_{int(std_factor*10)}"
            features[f'bb_upper_{idx}'] = bb_upper
            features[f'bb_middle_{idx}'] = bb_middle
            features[f'bb_lower_{idx}'] = bb_lower
            features[f'bb_position_{idx}'] = (close - bb_lower) / (bb_upper - bb_lower + 1e-8)
            features[f'bb_width_{idx}'] = (bb_upper - bb_lower) / bb_middle

        # ATR (å¤šå‘¨æœŸ)
        for period in [10, 14, 20]:
            features[f'atr_{period}'] = self._calculate_atr(high, low, close, period)

        # éšæœºæŒ‡æ ‡KD
        for period in [9, 14, 21]:
            k_percent, d_percent = self._calculate_stochastic(high, low, close, period)
            features[f'stoch_k_{period}'] = k_percent
            features[f'stoch_d_{period}'] = d_percent

        # å¨å»‰æŒ‡æ ‡
        for period in [10, 14, 20]:
            features[f'williams_r_{period}'] = self._calculate_williams_r(high, low, close, period)

        return features

    def _extract_volume_price_features(self, close, volume):
        """é‡ä»·å…³ç³»ç‰¹å¾"""
        features = {}

        # æˆäº¤é‡ç§»åŠ¨å¹³å‡
        for period in self.volume_periods:
            features[f'volume_ma_{period}'] = self._moving_average(volume, period)
            features[f'volume_ratio_{period}'] = volume / (self._moving_average(volume, period) + 1e-8)

        # é‡ä»·ç¡®è®¤æŒ‡æ ‡
        price_change = np.diff(close)
        volume_change = np.diff(volume)

        # OBV (On-Balance Volume)
        obv = np.cumsum(np.where(price_change > 0, volume[1:],
                                np.where(price_change < 0, -volume[1:], 0)))
        features['obv'] = obv

        # ä»·é‡æ•£åº¦
        for period in [10, 20]:
            if len(close) > period:
                price_ma = self._moving_average(close, period)
                volume_ma = self._moving_average(volume, period)
                features[f'pv_divergence_{period}'] = (close / price_ma) / (volume / volume_ma + 1e-8)

        return features

    def _extract_momentum_features(self, close, high, low):
        """åŠ¨é‡ç‰¹å¾"""
        features = {}

        # å¤šå‘¨æœŸåŠ¨é‡
        for period in self.momentum_periods:
            if len(close) > period:
                features[f'momentum_{period}'] = close[period:] / close[:-period] - 1

        # ROC (Rate of Change)
        for period in [10, 20, 50]:
            if len(close) > period:
                features[f'roc_{period}'] = (close[period:] - close[:-period]) / close[:-period]

        # ä»·æ ¼éœ‡è¡å¼ºåº¦
        for period in [10, 20]:
            if len(close) > period:
                price_range = high - low
                features[f'oscillation_{period}'] = self._moving_average(price_range, period) / close

        return features

    def _extract_volatility_modeling_features(self, close):
        """æ³¢åŠ¨ç‡å»ºæ¨¡ç‰¹å¾"""
        features = {}

        # å¯¹æ•°æ”¶ç›Šç‡
        log_returns = np.log(close[1:] / close[:-1])

        # å¤šå‘¨æœŸæ³¢åŠ¨ç‡
        for period in [5, 10, 20, 60]:
            if len(log_returns) > period:
                features[f'volatility_{period}'] = np.array([
                    np.std(log_returns[max(0, i-period+1):i+1])
                    for i in range(len(log_returns))
                ])

                # ååº¦å’Œå³°åº¦
                features[f'skewness_{period}'] = np.array([
                    self._calculate_skewness(log_returns[max(0, i-period+1):i+1])
                    for i in range(len(log_returns))
                ])

                features[f'kurtosis_{period}'] = np.array([
                    self._calculate_kurtosis(log_returns[max(0, i-period+1):i+1])
                    for i in range(len(log_returns))
                ])

        # GARCHç±»ç‰¹å¾
        features['garch_volatility'] = self._estimate_garch_volatility(log_returns)

        return features

    def _extract_microstructure_features(self, ohlcv_data):
        """å¸‚åœºå¾®è§‚ç»“æ„ç‰¹å¾"""
        close = ohlcv_data[:, 4]
        high = ohlcv_data[:, 2]
        low = ohlcv_data[:, 3]
        volume = ohlcv_data[:, 5]

        features = {}

        # ä»·æ ¼å½±å“å‡½æ•°
        log_returns = np.log(close[1:] / close[:-1])
        log_volume = np.log(volume[1:] + 1)

        # æµåŠ¨æ€§æŒ‡æ ‡
        features['amihud_illiquidity'] = np.abs(log_returns) / (log_volume + 1e-8)

        # ä¹°å–å‹åŠ›æŒ‡æ ‡
        typical_price = (high + low + close) / 3
        money_flow = typical_price * volume

        for period in [10, 20]:
            if len(money_flow) > period:
                positive_mf = np.where(np.diff(typical_price) > 0, money_flow[1:], 0)
                negative_mf = np.where(np.diff(typical_price) < 0, money_flow[1:], 0)

                features[f'mfi_{period}'] = np.array([
                    np.sum(positive_mf[max(0, i-period+1):i+1]) /
                    (np.sum(positive_mf[max(0, i-period+1):i+1]) + np.sum(negative_mf[max(0, i-period+1):i+1]) + 1e-8)
                    for i in range(len(positive_mf))
                ])

        return features

    def _extract_statistical_features(self, close, volume):
        """ç»Ÿè®¡ç‰¹å¾"""
        features = {}

        # å¤šé‡åˆ†å½¢ç‰¹å¾
        log_returns = np.log(close[1:] / close[:-1])

        for period in [20, 50]:
            if len(log_returns) > period:
                # è‡ªç›¸å…³
                features[f'autocorr_{period}'] = np.array([
                    np.corrcoef(log_returns[max(0, i-period+1):i],
                              log_returns[max(1, i-period+2):i+1])[0, 1] if i >= period else 0
                    for i in range(len(log_returns))
                ])

        return features

    def _extract_relative_strength_features(self, close):
        """ç›¸å¯¹å¼ºå¼±ç‰¹å¾"""
        features = {}

        # ç›¸å¯¹å¼ºå¼±æŒ‡æ•°
        for period in [10, 20, 50]:
            if len(close) > period:
                gains = np.where(np.diff(close) > 0, np.diff(close), 0)
                losses = np.where(np.diff(close) < 0, -np.diff(close), 0)

                avg_gain = self._moving_average(gains, period)
                avg_loss = self._moving_average(losses, period)

                rs = avg_gain / (avg_loss + 1e-8)
                features[f'rs_{period}'] = rs / (1 + rs)

        return features

    def _extract_risk_features(self, close):
        """é£é™©ç‰¹å¾"""
        features = {}

        log_returns = np.log(close[1:] / close[:-1])

        # VaR (Value at Risk)
        for period in [20, 50]:
            if len(log_returns) > period:
                features[f'var_95_{period}'] = np.array([
                    np.percentile(log_returns[max(0, i-period+1):i+1], 5)
                    for i in range(len(log_returns))
                ])

                features[f'var_99_{period}'] = np.array([
                    np.percentile(log_returns[max(0, i-period+1):i+1], 1)
                    for i in range(len(log_returns))
                ])

        # æœ€å¤§å›æ’¤
        for period in [20, 50, 100]:
            if len(close) > period:
                features[f'max_drawdown_{period}'] = np.array([
                    self._calculate_max_drawdown(close[max(0, i-period+1):i+1])
                    for i in range(len(close))
                ])

        return features

    def _extract_time_features(self, timestamp):
        """æ—¶é—´ç‰¹å¾ (ç”¨äºTime2Vec)"""
        features = {}

        # è½¬æ¢æ—¶é—´æˆ³
        dt_array = pd.to_datetime(timestamp, unit='s')

        features['hour'] = dt_array.hour.values
        features['day_of_week'] = dt_array.dayofweek.values
        features['day_of_month'] = dt_array.day.values
        features['month'] = dt_array.month.values
        features['quarter'] = dt_array.quarter.values

        # å‘¨æœŸæ€§æ—¶é—´ç‰¹å¾
        features['hour_sin'] = np.sin(2 * np.pi * features['hour'] / 24)
        features['hour_cos'] = np.cos(2 * np.pi * features['hour'] / 24)
        features['dow_sin'] = np.sin(2 * np.pi * features['day_of_week'] / 7)
        features['dow_cos'] = np.cos(2 * np.pi * features['day_of_week'] / 7)

        return features

    # è¾…åŠ©è®¡ç®—å‡½æ•°
    def _moving_average(self, data, period):
        """ç§»åŠ¨å¹³å‡"""
        return np.array([np.mean(data[max(0, i-period+1):i+1])
                        for i in range(len(data))])

    def _calculate_rsi(self, close, period):
        """RSIè®¡ç®—"""
        delta = np.diff(close)
        gain = np.where(delta > 0, delta, 0)
        loss = np.where(delta < 0, -delta, 0)

        avg_gain = self._moving_average(gain, period)
        avg_loss = self._moving_average(loss, period)

        rs = avg_gain / (avg_loss + 1e-8)
        return 100 - (100 / (1 + rs))

    def _calculate_macd(self, close, fast_period, slow_period, signal_period):
        """MACDè®¡ç®—"""
        ema_fast = self._exponential_moving_average(close, fast_period)
        ema_slow = self._exponential_moving_average(close, slow_period)

        macd = ema_fast - ema_slow
        macd_signal = self._exponential_moving_average(macd, signal_period)
        macd_histogram = macd - macd_signal

        return macd, macd_signal, macd_histogram

    def _exponential_moving_average(self, data, period):
        """æŒ‡æ•°ç§»åŠ¨å¹³å‡"""
        alpha = 2 / (period + 1)
        ema = np.zeros_like(data)
        ema[0] = data[0]

        for i in range(1, len(data)):
            ema[i] = alpha * data[i] + (1 - alpha) * ema[i-1]

        return ema

    def _calculate_bollinger_bands(self, close, period, std_factor):
        """å¸ƒæ—å¸¦è®¡ç®—"""
        sma = self._moving_average(close, period)
        std = np.array([np.std(close[max(0, i-period+1):i+1])
                       for i in range(len(close))])

        upper = sma + std_factor * std
        lower = sma - std_factor * std

        return upper, sma, lower

    def _calculate_atr(self, high, low, close, period):
        """ATRè®¡ç®—"""
        tr1 = high - low
        tr2 = np.abs(high - np.roll(close, 1))
        tr3 = np.abs(low - np.roll(close, 1))

        tr = np.maximum(tr1, np.maximum(tr2, tr3))
        return self._moving_average(tr, period)

    def _calculate_stochastic(self, high, low, close, period):
        """éšæœºæŒ‡æ ‡è®¡ç®—"""
        lowest_low = np.array([np.min(low[max(0, i-period+1):i+1])
                              for i in range(len(low))])
        highest_high = np.array([np.max(high[max(0, i-period+1):i+1])
                               for i in range(len(high))])

        k_percent = 100 * (close - lowest_low) / (highest_high - lowest_low + 1e-8)
        d_percent = self._moving_average(k_percent, 3)

        return k_percent, d_percent

    def _calculate_williams_r(self, high, low, close, period):
        """å¨å»‰æŒ‡æ ‡è®¡ç®—"""
        highest_high = np.array([np.max(high[max(0, i-period+1):i+1])
                               for i in range(len(high))])
        lowest_low = np.array([np.min(low[max(0, i-period+1):i+1])
                              for i in range(len(low))])

        return -100 * (highest_high - close) / (highest_high - lowest_low + 1e-8)

    def _calculate_skewness(self, data):
        """ååº¦è®¡ç®—"""
        if len(data) < 3:
            return 0
        mean = np.mean(data)
        std = np.std(data)
        if std == 0:
            return 0
        return np.mean(((data - mean) / std) ** 3)

    def _calculate_kurtosis(self, data):
        """å³°åº¦è®¡ç®—"""
        if len(data) < 4:
            return 0
        mean = np.mean(data)
        std = np.std(data)
        if std == 0:
            return 0
        return np.mean(((data - mean) / std) ** 4) - 3

    def _estimate_garch_volatility(self, returns, window=50):
        """ç®€åŒ–GARCHæ³¢åŠ¨ç‡ä¼°è®¡"""
        volatility = np.zeros_like(returns)

        for i in range(window, len(returns)):
            recent_returns = returns[i-window:i]
            volatility[i] = np.sqrt(np.mean(recent_returns**2))

        return volatility

    def _calculate_max_drawdown(self, prices):
        """æœ€å¤§å›æ’¤è®¡ç®—"""
        peak = np.maximum.accumulate(prices)
        drawdown = (prices - peak) / peak
        return np.min(drawdown)

    def create_sequences(self, features_dict: Dict[str, np.ndarray],
                        time_features: Dict[str, np.ndarray],
                        sequence_length: int = 60) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        ä¸šç•Œæœ€ä¼˜åºåˆ—åˆ›å»ºæ–¹æ³•
        è¿”å›: (features_X, time_X, valid_mask)
        """
        # å¯¹é½æ‰€æœ‰ç‰¹å¾çš„é•¿åº¦
        min_length = min(len(v) for v in features_dict.values())

        # å‡†å¤‡ä¸»è¦ç‰¹å¾
        aligned_features = []
        for name, values in features_dict.items():
            if 'hour' not in name and 'day' not in name and 'month' not in name and 'quarter' not in name:
                if len(values) == min_length:
                    aligned_features.append(values)

        # å‡†å¤‡æ—¶é—´ç‰¹å¾
        time_feature_list = ['hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'day_of_month', 'month', 'quarter']
        aligned_time_features = []
        for name in time_feature_list:
            if name in time_features and len(time_features[name]) == min_length:
                aligned_time_features.append(time_features[name])

        if not aligned_features or not aligned_time_features:
            return np.array([]), np.array([]), np.array([])

        # è½¬æ¢ä¸ºç‰¹å¾çŸ©é˜µ
        features_matrix = np.column_stack(aligned_features)
        time_matrix = np.column_stack(aligned_time_features)

        # åˆ›å»ºåºåˆ—
        features_X = []
        time_X = []
        valid_mask = []

        for i in range(sequence_length, len(features_matrix)):
            feature_seq = features_matrix[i-sequence_length:i]
            time_seq = time_matrix[i-sequence_length:i]

            features_X.append(feature_seq)
            time_X.append(time_seq)

            # åˆ›å»ºmaskï¼ˆè¿™é‡Œå‡è®¾æ‰€æœ‰æ•°æ®éƒ½æ˜¯æœ‰æ•ˆçš„ï¼‰
            mask = np.zeros(sequence_length, dtype=bool)
            valid_mask.append(mask)

        return np.array(features_X), np.array(time_X), np.array(valid_mask)

class IndustryLeadingTransformerTrainer:
    """
    ä¸šç•Œæœ€ä¼˜Transformerè®­ç»ƒå™¨
    æ”¯æŒæ–°çš„MultiStockTransformerModelï¼ŒåŒ…å«é«˜çº§è®­ç»ƒæŠ€å·§
    """

    def __init__(self, model: MultiStockTransformerModel, device: str = 'cpu'):
        self.model = model
        self.device = device
        self.model.to(device)

        # ä¸šç•Œæœ€ä¼˜ä¼˜åŒ–å™¨é…ç½®
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=2e-4,  # ç•¥å¾®æé«˜å­¦ä¹ ç‡
            weight_decay=0.01,
            betas=(0.9, 0.999),  # æ ‡å‡†é…ç½®
            eps=1e-8
        )

        # é¢„çƒ­+ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦
        self.warmup_steps = 1000
        self.total_steps = 50000
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer, T_0=1000, T_mult=2, eta_min=1e-6
        )

        # ä¸šç•Œæ ‡å‡†æŸå¤±å‡½æ•°
        self.direction_loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)
        self.regression_loss_fn = nn.HuberLoss(delta=1.0)  # å¯¹å¼‚å¸¸å€¼æ›´é²æ£’
        self.confidence_loss_fn = nn.BCELoss()  # ä¸“é—¨ç”¨äºç½®ä¿¡åº¦

        # æŸå¤±æƒé‡ï¼ˆåŸºäºä¸šç•Œæœ€ä½³å®è·µï¼‰
        self.loss_weights = {
            'direction': 1.0,
            'volatility': 0.3,
            'confidence': 0.4,
            'expected_return': 0.6,
            'sharpe_ratio': 0.2
        }

        # æ—©åœæœºåˆ¶
        self.best_loss = float('inf')
        self.patience = 0
        self.max_patience = 20

        # è®­ç»ƒç»Ÿè®¡
        self.step_count = 0
        self.epoch_count = 0

    def train_step(self,
                  features: torch.Tensor,
                  time_features: torch.Tensor,
                  targets: Dict[str, torch.Tensor],
                  stock_ids: Optional[torch.Tensor] = None,
                  mask: Optional[torch.Tensor] = None) -> Dict[str, float]:
        """
        ä¸šç•Œæœ€ä¼˜å•æ­¥è®­ç»ƒ
        è¿”å›è¯¦ç»†çš„æŸå¤±ä¿¡æ¯
        """
        self.model.train()

        # æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
        features = features.to(self.device)
        time_features = time_features.to(self.device)

        targets_device = {}
        for key, value in targets.items():
            targets_device[key] = value.to(self.device)

        if stock_ids is not None:
            stock_ids = stock_ids.to(self.device)
        if mask is not None:
            mask = mask.to(self.device)

        # å‰å‘ä¼ æ’­
        outputs = self.model(features, time_features, stock_ids, mask)

        # è®¡ç®—å„é¡¹æŸå¤±
        losses = {}
        losses['direction'] = self.direction_loss_fn(
            outputs['direction'], targets_device['direction']
        )
        losses['volatility'] = self.regression_loss_fn(
            outputs['volatility'], targets_device['volatility']
        )
        losses['confidence'] = self.confidence_loss_fn(
            outputs['confidence'], targets_device['confidence']
        )
        losses['expected_return'] = self.regression_loss_fn(
            outputs['expected_return'], targets_device['expected_return']
        )
        losses['sharpe_ratio'] = self.regression_loss_fn(
            outputs['sharpe_ratio'], targets_device['sharpe_ratio']
        )

        # åŠ æƒæ€»æŸå¤±
        total_loss = sum(
            self.loss_weights[key] * loss
            for key, loss in losses.items()
        )

        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        total_loss.backward()

        # æ¢¯åº¦è£å‰ªï¼ˆä¸šç•Œæ ‡å‡†ï¼‰
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)

        self.optimizer.step()
        self.scheduler.step()

        self.step_count += 1

        # è¿”å›æŸå¤±ä¿¡æ¯
        loss_dict = {f'{key}_loss': loss.item() for key, loss in losses.items()}
        loss_dict['total_loss'] = total_loss.item()
        loss_dict['learning_rate'] = self.optimizer.param_groups[0]['lr']

        return loss_dict

    def evaluate(self,
                features: torch.Tensor,
                time_features: torch.Tensor,
                targets: Dict[str, torch.Tensor],
                stock_ids: Optional[torch.Tensor] = None,
                mask: Optional[torch.Tensor] = None) -> Dict[str, float]:
        """
        ä¸šç•Œæ ‡å‡†æ¨¡å‹è¯„ä¼°
        è¿”å›è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡
        """
        self.model.eval()
        with torch.no_grad():
            # æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
            features = features.to(self.device)
            time_features = time_features.to(self.device)

            targets_device = {}
            for key, value in targets.items():
                targets_device[key] = value.to(self.device)

            if stock_ids is not None:
                stock_ids = stock_ids.to(self.device)
            if mask is not None:
                mask = mask.to(self.device)

            outputs = self.model(features, time_features, stock_ids, mask)

            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            metrics = {}

            # åˆ†ç±»å‡†ç¡®ç‡
            direction_pred = torch.argmax(outputs['direction'], dim=1)
            metrics['direction_accuracy'] = (
                direction_pred == targets_device['direction']
            ).float().mean().item()

            # å›å½’æŒ‡æ ‡
            metrics['volatility_mae'] = torch.mean(
                torch.abs(outputs['volatility'] - targets_device['volatility'])
            ).item()

            metrics['return_mae'] = torch.mean(
                torch.abs(outputs['expected_return'] - targets_device['expected_return'])
            ).item()

            # ç½®ä¿¡åº¦æ ¡å‡†
            confidence_diff = torch.abs(
                outputs['confidence'] - targets_device['confidence']
            )
            metrics['confidence_calibration'] = torch.mean(confidence_diff).item()

            # å¤æ™®æ¯”ç‡é¢„æµ‹ç²¾åº¦
            sharpe_diff = torch.abs(
                outputs['sharpe_ratio'] - targets_device['sharpe_ratio']
            )
            metrics['sharpe_mae'] = torch.mean(sharpe_diff).item()

            # æ•´ä½“æ€§èƒ½æŒ‡æ ‡
            direction_weight = 0.4
            regression_weight = 0.6

            metrics['overall_score'] = (
                direction_weight * metrics['direction_accuracy'] +
                regression_weight * (1.0 - min(metrics['return_mae'], 1.0))
            )

            return metrics

    def save_model(self, path: str, include_optimizer: bool = True):
        """ä¿å­˜æ¨¡å‹"""
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'step_count': self.step_count,
            'epoch_count': self.epoch_count,
            'best_loss': self.best_loss,
        }

        if include_optimizer:
            checkpoint['optimizer_state_dict'] = self.optimizer.state_dict()
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()

        torch.save(checkpoint, path)

    def load_model(self, path: str, load_optimizer: bool = True):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path, map_location=self.device)

        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.step_count = checkpoint.get('step_count', 0)
        self.epoch_count = checkpoint.get('epoch_count', 0)
        self.best_loss = checkpoint.get('best_loss', float('inf'))

        if load_optimizer and 'optimizer_state_dict' in checkpoint:
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

    def should_stop_early(self, current_loss: float) -> bool:
        """æ—©åœæ£€æŸ¥"""
        if current_loss < self.best_loss:
            self.best_loss = current_loss
            self.patience = 0
            return False
        else:
            self.patience += 1
            return self.patience >= self.max_patience

if __name__ == "__main__":
    # æµ‹è¯•ä¸šç•Œæœ€ä¼˜æ¨¡å‹
    print("ğŸš€ æµ‹è¯•ä¸šç•Œæœ€ä¼˜MultiStockTransformerModel...")

    # æ£€æŸ¥è®¾å¤‡
    if torch.backends.mps.is_available():
        device = 'mps'
        print("âœ… ä½¿ç”¨Metal Performance Shaders (MPS)")
    elif torch.cuda.is_available():
        device = 'cuda'
        print("âœ… ä½¿ç”¨CUDA")
    else:
        device = 'cpu'
        print("âœ… ä½¿ç”¨CPU")

    # åˆ›å»ºä¸šç•Œæœ€ä¼˜æ¨¡å‹
    model = MultiStockTransformerModel(
        input_dim=50,
        time_dim=8,
        d_model=256,
        nhead=16,
        num_layers=6,
        num_stocks=10,
        enable_cross_stock=True
    )
    trainer = IndustryLeadingTransformerTrainer(model, device)

    print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")

    # æµ‹è¯•æ•°æ®
    batch_size, seq_len, input_dim, time_dim = 4, 60, 50, 8
    test_features = torch.randn(batch_size, seq_len, input_dim)
    test_time_features = torch.randn(batch_size, seq_len, time_dim)
    test_stock_ids = torch.randint(0, 10, (batch_size,))

    test_targets = {
        'direction': torch.randint(0, 3, (batch_size,)),
        'volatility': torch.rand(batch_size, 1),  # éè´Ÿ
        'confidence': torch.rand(batch_size, 1),  # 0-1
        'expected_return': torch.randn(batch_size, 1),
        'sharpe_ratio': torch.randn(batch_size, 1)
    }

    # å°†æµ‹è¯•æ•°æ®ç§»åˆ°æ­£ç¡®è®¾å¤‡
    test_features = test_features.to(device)
    test_time_features = test_time_features.to(device)
    test_stock_ids = test_stock_ids.to(device)
    for key in test_targets:
        test_targets[key] = test_targets[key].to(device)

    # å‰å‘ä¼ æ’­æµ‹è¯•
    outputs = model(test_features, test_time_features, test_stock_ids)
    print(f"âœ… æ¨¡å‹è¾“å‡ºç»´åº¦æ£€æŸ¥:")
    for key, value in outputs.items():
        print(f"  {key}: {value.shape}")

    # è®­ç»ƒæ­¥éª¤æµ‹è¯•
    loss_dict = trainer.train_step(test_features, test_time_features, test_targets, test_stock_ids)
    print(f"âœ… è®­ç»ƒæ­¥éª¤æµ‹è¯•å®Œæˆ:")
    for key, value in loss_dict.items():
        print(f"  {key}: {value:.4f}")

    # è¯„ä¼°æµ‹è¯•
    metrics = trainer.evaluate(test_features, test_time_features, test_targets, test_stock_ids)
    print(f"âœ… è¯„ä¼°æµ‹è¯•å®Œæˆ:")
    for key, value in metrics.items():
        print(f"  {key}: {value:.4f}")

if __name__ == "__main__":
    print("ğŸ¤– Transformeræ¨¡å‹åº“åŠ è½½æˆåŠŸ!")
    print("ğŸ“š å¯ç”¨ç»„ä»¶:")
    print("  - MultiStockTransformerModel")
    print("  - IndustryLeadingFeatureExtractor")
    print("  - IndustryLeadingTransformerTrainer")
    print("  - Time2Vec")
    print("  - AdvancedPositionalEncoding")

