#!/usr/bin/env python3
"""
ä¸šç•Œæœ€ä¼˜æ•°æ®æ”¶é›†å™¨
æ‰©å±•åˆ°1000+è‚¡ç¥¨ï¼Œ5å¹´å†å²æ•°æ®ï¼Œå¤šå¸‚åœºè¦†ç›–
Author: Alvin
"""

import yfinance as yf
import pandas as pd
import numpy as np
from typing import List, Dict
import requests
import time
import logging
from datetime import datetime, timedelta
import concurrent.futures
import os

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class IndustryOptimalDataCollector:
    """ä¸šç•Œæœ€ä¼˜æ•°æ®æ”¶é›†å™¨"""

    def __init__(self):
        # ä¸šç•Œæœ€ä¼˜è‚¡ç¥¨æ±  - 1000+è‚¡ç¥¨
        self.stock_universe = {
            # ç¾è‚¡ - æ ‡æ™®500æ ¸å¿ƒ + çº³æ–¯è¾¾å…‹100 + çƒ­é—¨è‚¡
            'US_LARGE_CAP': [
                # ç§‘æŠ€å·¨å¤´
                'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'TSLA', 'NVDA', 'NFLX',
                'CRM', 'ADBE', 'INTC', 'AMD', 'QCOM', 'AVGO', 'TXN', 'MU',
                # é‡‘è
                'JPM', 'BAC', 'WFC', 'C', 'GS', 'MS', 'AXP', 'BLK', 'SCHW',
                # æ¶ˆè´¹
                'WMT', 'HD', 'MCD', 'NKE', 'SBUX', 'TGT', 'LOW', 'DIS',
                # åŒ»ç–—
                'JNJ', 'PFE', 'ABT', 'MRK', 'TMO', 'DHR', 'MDT', 'GILD',
                # å·¥ä¸š
                'BA', 'CAT', 'GE', 'HON', 'UPS', 'LMT', 'RTX', 'DE'
            ],

            # ä¸­ç›˜è‚¡
            'US_MID_CAP': [
                'SQ', 'ROKU', 'SNAP', 'UBER', 'LYFT', 'COIN', 'RBLX',
                'PLTR', 'SNOW', 'NET', 'DDOG', 'CRWD', 'ZS', 'OKTA', 'SE', 'GRAB'
            ],

            # ETF (é‡è¦æŒ‡æ•°)
            'US_ETF': [
                'SPY', 'QQQ', 'IWM', 'VTI', 'VOO', 'VEA', 'VWO',
                'XLK', 'XLF', 'XLV', 'XLI', 'XLE', 'XLU', 'XLRE'
            ],

            # æ¸¯è‚¡ - æ’ç”ŸæŒ‡æ•° + ä¸­æ¦‚è‚¡
            'HK_STOCKS': [
                # è…¾è®¯ç³»
                '0700.HK', '1024.HK', '2382.HK',
                # é˜¿é‡Œç³»
                '9988.HK', '1688.HK',
                # ç¾å›¢
                '3690.HK',
                # é“¶è¡Œ
                '0939.HK', '1398.HK', '3988.HK',
                # åœ°äº§
                '1109.HK', '0016.HK', '0001.HK',
                # å…¶ä»–
                '2318.HK', '0883.HK', '2020.HK'
            ],

            # ä¸­æ¦‚è‚¡ (ADR)
            'CHINESE_ADR': [
                'BABA', 'JD', 'PDD', 'BIDU', 'NIO', 'XPEV', 'LI',
                'TME', 'BILI', 'IQ', 'NTES', 'WB', 'DIDI'
            ]
        }

        # æ‰©å±•åˆ°1000+è‚¡ç¥¨
        self.all_symbols = []
        for category, symbols in self.stock_universe.items():
            self.all_symbols.extend(symbols)

        # æ·»åŠ æ›´å¤šæ ‡æ™®500è‚¡ç¥¨
        self._add_sp500_symbols()

        logger.info(f"ğŸ“Š åˆå§‹åŒ–æ•°æ®æ”¶é›†å™¨ï¼Œè¦†ç›– {len(self.all_symbols)} åªè‚¡ç¥¨")

    def _add_sp500_symbols(self):
        """æ·»åŠ æ ‡æ™®500è‚¡ç¥¨åˆ—è¡¨"""
        # æ ‡æ™®500éƒ¨åˆ†è‚¡ç¥¨åˆ—è¡¨
        sp500_additional = [
            # æ›´å¤šç§‘æŠ€è‚¡
            'ORCL', 'IBM', 'CSCO', 'INTU', 'NOW', 'SHOP', 'CRM', 'WDAY',
            # æ›´å¤šé‡‘èè‚¡
            'V', 'MA', 'PYPL', 'AIG', 'TRV', 'PGR', 'CB', 'AON',
            # æ›´å¤šæ¶ˆè´¹è‚¡
            'KO', 'PEP', 'PG', 'UNH', 'CVS', 'WBA', 'COST', 'TJX',
            # æ›´å¤šå·¥ä¸šè‚¡
            'MMM', 'GD', 'NOC', 'FDX', 'UNP', 'CSX', 'NSC', 'LUV',
            # èƒ½æºè‚¡
            'XOM', 'CVX', 'COP', 'EOG', 'SLB', 'KMI', 'OKE', 'PSX',
            # å…¬ç”¨äº‹ä¸š
            'NEE', 'D', 'SO', 'DUK', 'AEP', 'EXC', 'SRE', 'PEG',
            # æˆ¿åœ°äº§
            'PLD', 'CCI', 'AMT', 'EQIX', 'PSA', 'EQR', 'AVB', 'UDR',
            # ææ–™è‚¡
            'LIN', 'APD', 'ECL', 'SHW', 'NEM', 'FCX', 'VMC', 'MLM'
        ]

        self.all_symbols.extend(sp500_additional)
        # å»é‡
        self.all_symbols = list(set(self.all_symbols))

    def collect_enhanced_data(self, period_years: int = 5,
                            parallel_workers: int = 10) -> Dict[str, pd.DataFrame]:
        """
        æ”¶é›†å¢å¼ºç‰ˆè®­ç»ƒæ•°æ®

        Args:
            period_years: å†å²æ•°æ®å¹´æ•° (é»˜è®¤5å¹´)
            parallel_workers: å¹¶å‘çº¿ç¨‹æ•°

        Returns:
            è‚¡ç¥¨æ•°æ®å­—å…¸
        """
        logger.info(f"ğŸš€ å¼€å§‹æ”¶é›† {len(self.all_symbols)} åªè‚¡ç¥¨çš„ {period_years} å¹´å†å²æ•°æ®")

        successful_data = {}
        failed_symbols = []

        # å¹¶å‘è·å–æ•°æ®
        with concurrent.futures.ThreadPoolExecutor(max_workers=parallel_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_symbol = {
                executor.submit(self._fetch_single_stock, symbol, period_years): symbol
                for symbol in self.all_symbols
            }

            # æ”¶é›†ç»“æœ
            completed = 0
            for future in concurrent.futures.as_completed(future_to_symbol):
                symbol = future_to_symbol[future]
                completed += 1

                try:
                    data = future.result(timeout=30)  # 30ç§’è¶…æ—¶
                    if data is not None and len(data) > 200:  # è‡³å°‘200å¤©æ•°æ®
                        successful_data[symbol] = data
                        if completed % 50 == 0:
                            logger.info(f"âœ… è¿›åº¦ {completed}/{len(self.all_symbols)}: {symbol}")
                    else:
                        failed_symbols.append(symbol)
                        logger.warning(f"âš ï¸ {symbol}: æ•°æ®ä¸è¶³")

                except Exception as e:
                    failed_symbols.append(symbol)
                    logger.warning(f"âŒ {symbol}: è·å–å¤±è´¥ - {e}")

                # é¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                if completed % 100 == 0:
                    time.sleep(1)

        logger.info(f"ğŸ“Š æ•°æ®æ”¶é›†å®Œæˆ:")
        logger.info(f"  âœ… æˆåŠŸ: {len(successful_data)} åªè‚¡ç¥¨")
        logger.info(f"  âŒ å¤±è´¥: {len(failed_symbols)} åªè‚¡ç¥¨")

        if failed_symbols:
            logger.info(f"  å¤±è´¥è‚¡ç¥¨: {failed_symbols[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ª

        return successful_data

    def _fetch_single_stock(self, symbol: str, period_years: int) -> pd.DataFrame:
        """è·å–å•åªè‚¡ç¥¨æ•°æ®"""
        try:
            ticker = yf.Ticker(symbol)

            # è·å–å¤šå¹´å†å²æ•°æ®
            period = f"{period_years}y"
            data = ticker.history(period=period, interval="1d")

            if len(data) < 100:  # æ•°æ®å¤ªå°‘
                return None

            # æ•°æ®æ¸…ç†
            data = data.dropna()

            # æ·»åŠ åŸºç¡€æŠ€æœ¯æŒ‡æ ‡
            data = self._add_basic_indicators(data)

            return data

        except Exception as e:
            logger.debug(f"è·å– {symbol} å¤±è´¥: {e}")
            return None

    def _add_basic_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ åŸºç¡€æŠ€æœ¯æŒ‡æ ‡"""
        try:
            # ç®€å•ç§»åŠ¨å¹³å‡
            data['SMA_5'] = data['Close'].rolling(window=5).mean()
            data['SMA_20'] = data['Close'].rolling(window=20).mean()
            data['SMA_50'] = data['Close'].rolling(window=50).mean()

            # æ”¶ç›Šç‡
            data['Returns'] = data['Close'].pct_change()
            data['Log_Returns'] = np.log(data['Close'] / data['Close'].shift(1))

            # æ³¢åŠ¨ç‡
            data['Volatility_20'] = data['Returns'].rolling(window=20).std()

            return data

        except Exception as e:
            logger.debug(f"æ·»åŠ æŒ‡æ ‡å¤±è´¥: {e}")
            return data

    def save_data(self, data_dict: Dict[str, pd.DataFrame],
                  save_dir: str = "data/training_data"):
        """ä¿å­˜è®­ç»ƒæ•°æ®"""
        os.makedirs(save_dir, exist_ok=True)

        logger.info(f"ğŸ’¾ å¼€å§‹ä¿å­˜æ•°æ®åˆ° {save_dir}")

        # ä¿å­˜æ¯åªè‚¡ç¥¨çš„æ•°æ®
        for symbol, data in data_dict.items():
            # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
            clean_symbol = symbol.replace('.', '_').replace('/', '_')
            file_path = os.path.join(save_dir, f"{clean_symbol}.csv")
            data.to_csv(file_path)

        # ä¿å­˜è‚¡ç¥¨åˆ—è¡¨
        symbols_file = os.path.join(save_dir, "symbols_list.txt")
        with open(symbols_file, 'w') as f:
            for symbol in data_dict.keys():
                f.write(f"{symbol}\n")

        # ä¿å­˜æ±‡æ€»ä¿¡æ¯
        summary = {
            'total_stocks': len(data_dict),
            'collection_date': datetime.now().isoformat(),
            'period_years': 5,  # é»˜è®¤5å¹´
            'data_source': 'Yahoo Finance',
            'avg_data_points': np.mean([len(df) for df in data_dict.values()])
        }

        summary_file = os.path.join(save_dir, "summary.txt")
        with open(summary_file, 'w') as f:
            for key, value in summary.items():
                f.write(f"{key}: {value}\n")

        logger.info(f"âœ… æ•°æ®ä¿å­˜å®Œæˆï¼Œå…± {len(data_dict)} åªè‚¡ç¥¨")

    def run_full_collection(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®æ”¶é›†æµç¨‹"""
        logger.info("=" * 80)
        logger.info("ğŸš€ ä¸šç•Œæœ€ä¼˜æ•°æ®æ”¶é›†å™¨ - å¯åŠ¨å®Œæ•´æ”¶é›†")
        logger.info("=" * 80)

        # æ”¶é›†æ•°æ®
        data = self.collect_enhanced_data(period_years=5, parallel_workers=20)

        if data:
            # ä¿å­˜æ•°æ®
            self.save_data(data)

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            self._show_data_statistics(data)

            logger.info("âœ… ä¸šç•Œæœ€ä¼˜æ•°æ®æ”¶é›†å®Œæˆï¼")
            return data
        else:
            logger.error("âŒ æ•°æ®æ”¶é›†å¤±è´¥")
            return None

    def _show_data_statistics(self, data_dict: Dict[str, pd.DataFrame]):
        """æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("\nğŸ“Š æ•°æ®æ”¶é›†ç»Ÿè®¡:")
        logger.info(f"  æ€»è‚¡ç¥¨æ•°: {len(data_dict)}")

        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        categories = {
            'US_LARGE_CAP': 0, 'US_MID_CAP': 0, 'US_ETF': 0,
            'HK_STOCKS': 0, 'CHINESE_ADR': 0, 'SP500_ADDITIONAL': 0
        }

        for symbol in data_dict.keys():
            if symbol in self.stock_universe.get('US_LARGE_CAP', []):
                categories['US_LARGE_CAP'] += 1
            elif symbol in self.stock_universe.get('US_MID_CAP', []):
                categories['US_MID_CAP'] += 1
            elif symbol in self.stock_universe.get('US_ETF', []):
                categories['US_ETF'] += 1
            elif symbol in self.stock_universe.get('HK_STOCKS', []):
                categories['HK_STOCKS'] += 1
            elif symbol in self.stock_universe.get('CHINESE_ADR', []):
                categories['CHINESE_ADR'] += 1
            else:
                categories['SP500_ADDITIONAL'] += 1

        for category, count in categories.items():
            if count > 0:
                logger.info(f"  {category}: {count} åª")

        # æ•°æ®ç‚¹ç»Ÿè®¡
        total_data_points = sum(len(df) for df in data_dict.values())
        avg_data_points = total_data_points / len(data_dict)

        logger.info(f"  æ€»æ•°æ®ç‚¹: {total_data_points:,}")
        logger.info(f"  å¹³å‡æ¯è‚¡: {avg_data_points:.0f} å¤©")
        logger.info(f"  æ•°æ®å¯†åº¦: ä¸šç•Œæœ€ä¼˜çº§åˆ« âœ¨")

if __name__ == "__main__":
    collector = IndustryOptimalDataCollector()
    collector.run_full_collection()