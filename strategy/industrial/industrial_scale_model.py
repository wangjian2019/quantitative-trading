#!/usr/bin/env python3
"""
å·¥ä¸šçº§å¤§è§„æ¨¡é‡åŒ–äº¤æ˜“AIæ¨¡å‹
ä¸“ä¸ºGPUé›†ç¾¤è®¾è®¡çš„é«˜æ€§èƒ½æ¶æ„

ä½œè€…: Alvin
ç›®æ ‡: 1000åªçƒ­é—¨ç¾è‚¡æ¸¯è‚¡ + 5å¹´å†å²æ•°æ® + æœ€å…ˆè¿›çš„æ¨¡å‹æ¶æ„
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler
import numpy as np
import pandas as pd
import math
from typing import Optional, Tuple, List, Dict
import warnings
warnings.filterwarnings("ignore")

class PositionalEncoding(nn.Module):
    """å¢å¼ºä½ç½®ç¼–ç """
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        position = torch.arange(max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                           (-math.log(10000.0) / d_model))

        pe = torch.zeros(max_len, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)

class Time2Vec(nn.Module):
    """Time2Vecæ—¶é—´ç¼–ç å±‚"""
    def __init__(self, input_dim: int, time_dim: int = 64):
        super().__init__()
        self.time_dim = time_dim

        # çº¿æ€§å˜æ¢å±‚
        self.linear_layer = nn.Linear(input_dim, 1)

        # å‘¨æœŸæ€§å˜æ¢å±‚
        self.periodic_layers = nn.ModuleList([
            nn.Linear(input_dim, 1) for _ in range(time_dim - 1)
        ])

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # çº¿æ€§éƒ¨åˆ†
        linear_output = self.linear_layer(x)

        # å‘¨æœŸæ€§éƒ¨åˆ†
        periodic_outputs = []
        for layer in self.periodic_layers:
            periodic_outputs.append(torch.sin(layer(x)))

        # æ‹¼æ¥
        time_encoding = torch.cat([linear_output] + periodic_outputs, dim=-1)
        return time_encoding

class MultiHeadCrossStockAttention(nn.Module):
    """å¤šå¤´è·¨è‚¡ç¥¨æ³¨æ„åŠ›æœºåˆ¶"""
    def __init__(self, d_model: int, num_heads: int = 16, dropout: float = 0.1):
        super().__init__()
        assert d_model % num_heads == 0

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)

        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(d_model)

    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)

        output = torch.matmul(attention_weights, V)
        return output, attention_weights

    def forward(self, query, key, value, mask=None):
        batch_size, seq_len = query.size(0), query.size(1)

        # çº¿æ€§å˜æ¢
        Q = self.w_q(query).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = self.w_k(key).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = self.w_v(value).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)

        # æ³¨æ„åŠ›è®¡ç®—
        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)

        # æ‹¼æ¥å¤šå¤´
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.d_model)

        # æœ€ç»ˆçº¿æ€§å˜æ¢
        output = self.w_o(attention_output)

        # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        output = self.layer_norm(output + query)

        return output, attention_weights

class IndustryLeadingFeatureExtractor(nn.Module):
    """å·¥ä¸šçº§ç‰¹å¾æå–å™¨"""
    def __init__(self, raw_features: int = 200):
        super().__init__()

        # æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾æå–
        self.technical_extractor = nn.Sequential(
            nn.Linear(raw_features, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.GELU(),
            nn.Dropout(0.1)
        )

        # ä»·æ ¼åŠ¨é‡ç‰¹å¾
        self.momentum_extractor = nn.Sequential(
            nn.Conv1d(in_channels=raw_features, out_channels=128, kernel_size=3, padding=1),
            nn.BatchNorm1d(128),
            nn.GELU(),
            nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, padding=1),
            nn.BatchNorm1d(64),
            nn.GELU(),
            nn.AdaptiveAvgPool1d(1)
        )

        # æ³¢åŠ¨ç‡ç‰¹å¾
        self.volatility_extractor = nn.Sequential(
            nn.Linear(raw_features, 128),
            nn.LayerNorm(128),
            nn.GELU(),
            nn.Linear(128, 64),
            nn.LayerNorm(64),
            nn.GELU()
        )

        # ç‰¹å¾èåˆå±‚
        self.feature_fusion = nn.Sequential(
            nn.Linear(256 + 64 + 64, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(512, 256)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        batch_size, seq_len, features = x.shape

        # æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
        tech_features = self.technical_extractor(x)  # [B, T, 256]

        # åŠ¨é‡ç‰¹å¾ (éœ€è¦è½¬æ¢ç»´åº¦)
        x_transposed = x.transpose(1, 2)  # [B, F, T]
        momentum_features = self.momentum_extractor(x_transposed)  # [B, 64, 1]
        momentum_features = momentum_features.squeeze(-1).unsqueeze(1)  # [B, 1, 64]
        momentum_features = momentum_features.expand(-1, seq_len, -1)  # [B, T, 64]

        # æ³¢åŠ¨ç‡ç‰¹å¾
        volatility_features = self.volatility_extractor(x)  # [B, T, 64]

        # ç‰¹å¾èåˆ
        combined_features = torch.cat([tech_features, momentum_features, volatility_features], dim=-1)
        final_features = self.feature_fusion(combined_features)

        return final_features

class AdvancedTransformerBlock(nn.Module):
    """é«˜çº§Transformeræ¨¡å—"""
    def __init__(self, d_model: int, num_heads: int, dim_feedforward: int, dropout: float = 0.1):
        super().__init__()

        # å¤šå¤´æ³¨æ„åŠ›
        self.self_attention = nn.MultiheadAttention(
            d_model, num_heads, dropout=dropout, batch_first=True
        )

        # è·¨è‚¡ç¥¨æ³¨æ„åŠ›
        self.cross_stock_attention = MultiHeadCrossStockAttention(d_model, num_heads, dropout)

        # å‰é¦ˆç½‘ç»œ
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, dim_feedforward),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(dim_feedforward, d_model)
        )

        # å±‚å½’ä¸€åŒ–
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)

        # Dropout
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor, src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # è‡ªæ³¨æ„åŠ›
        attn_output, _ = self.self_attention(x, x, x, attn_mask=src_mask)
        x = self.norm1(x + self.dropout(attn_output))

        # è·¨è‚¡ç¥¨æ³¨æ„åŠ›
        cross_attn_output, _ = self.cross_stock_attention(x, x, x)
        x = self.norm2(x + cross_attn_output)

        # å‰é¦ˆç½‘ç»œ
        ff_output = self.feed_forward(x)
        x = self.norm3(x + self.dropout(ff_output))

        return x

class IndustryLeadingTransformer(nn.Module):
    """å·¥ä¸šçº§å¤§è§„æ¨¡Transformeræ¨¡å‹"""
    def __init__(
        self,
        num_stocks: int = 1000,
        raw_features: int = 200,
        d_model: int = 1024,
        num_heads: int = 16,
        num_layers: int = 12,
        dim_feedforward: int = 4096,
        seq_len: int = 252,  # ä¸€å¹´äº¤æ˜“æ—¥
        dropout: float = 0.1,
        num_tasks: int = 5,  # å¤šä»»åŠ¡å­¦ä¹ 
    ):
        super().__init__()

        self.num_stocks = num_stocks
        self.d_model = d_model
        self.seq_len = seq_len

        # è‚¡ç¥¨åµŒå…¥
        self.stock_embedding = nn.Embedding(num_stocks, d_model // 4)

        # ç‰¹å¾æå–å™¨
        self.feature_extractor = IndustryLeadingFeatureExtractor(raw_features)

        # Time2Vecæ—¶é—´ç¼–ç 
        self.time2vec = Time2Vec(256, d_model // 4)

        # ç‰¹å¾æŠ•å½±å±‚
        self.feature_projection = nn.Linear(256, d_model // 2)

        # ç‰¹å¾èåˆå±‚
        self.feature_fusion = nn.Linear(d_model, d_model)

        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(d_model, dropout, seq_len)

        # Transformerå±‚
        self.transformer_layers = nn.ModuleList([
            AdvancedTransformerBlock(d_model, num_heads, dim_feedforward, dropout)
            for _ in range(num_layers)
        ])

        # å…¨å±€å¹³å‡æ± åŒ–
        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)

        # å¤šä»»åŠ¡é¢„æµ‹å¤´
        self.task_heads = nn.ModuleDict({
            'direction': self._create_prediction_head(d_model, 3),      # æ¶¨/è·Œ/æ¨ªç›˜
            'volatility': self._create_prediction_head(d_model, 1),     # æ³¢åŠ¨ç‡é¢„æµ‹
            'return': self._create_prediction_head(d_model, 1),         # æ”¶ç›Šç‡é¢„æµ‹
            'confidence': self._create_prediction_head(d_model, 1),     # ç½®ä¿¡åº¦
            'risk_level': self._create_prediction_head(d_model, 5),     # é£é™©ç­‰çº§
        })

        # åˆå§‹åŒ–æƒé‡
        self._init_weights()

    def _create_prediction_head(self, d_model: int, output_dim: int) -> nn.Module:
        return nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.LayerNorm(d_model // 2),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(d_model // 2, d_model // 4),
            nn.LayerNorm(d_model // 4),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(d_model // 4, output_dim)
        )

    def _init_weights(self):
        """æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.LayerNorm):
                nn.init.ones_(module.weight)
                nn.init.zeros_(module.bias)

    def forward(self, x: torch.Tensor, stock_ids: torch.Tensor) -> Dict[str, torch.Tensor]:
        batch_size, seq_len, _ = x.shape

        # ç‰¹å¾æå–
        features = self.feature_extractor(x)  # [B, T, 256]

        # Time2Vecç¼–ç 
        time_features = self.time2vec(features)  # [B, T, d_model//4]

        # è‚¡ç¥¨åµŒå…¥
        stock_emb = self.stock_embedding(stock_ids)  # [B, d_model//4]
        stock_emb = stock_emb.unsqueeze(1).expand(-1, seq_len, -1)  # [B, T, d_model//4]

        # ç‰¹å¾æŠ•å½±
        projected_features = self.feature_projection(features)  # [B, T, d_model//2]

        # ç‰¹å¾æ‹¼æ¥
        combined_features = torch.cat([
            projected_features,
            time_features,
            stock_emb
        ], dim=-1)  # [B, T, d_model]

        # ç‰¹å¾èåˆ
        x = self.feature_fusion(combined_features)

        # ä½ç½®ç¼–ç 
        x = self.pos_encoding(x)

        # Transformerå±‚
        for layer in self.transformer_layers:
            x = layer(x)

        # å…¨å±€è¡¨ç¤º - ä½¿ç”¨å¤šç§èšåˆæ–¹å¼
        # 1. æœ€åæ—¶é—´æ­¥
        last_hidden = x[:, -1, :]

        # 2. å¹³å‡æ± åŒ–
        avg_hidden = torch.mean(x, dim=1)

        # 3. æœ€å¤§æ± åŒ–
        max_hidden, _ = torch.max(x, dim=1)

        # 4. æ³¨æ„åŠ›åŠ æƒæ± åŒ–
        attention_weights = F.softmax(
            torch.sum(x * x[:, -1:, :], dim=-1), dim=1
        )  # [B, T]
        weighted_hidden = torch.sum(
            x * attention_weights.unsqueeze(-1), dim=1
        )  # [B, d_model]

        # ç»„åˆæ‰€æœ‰è¡¨ç¤º
        final_representation = (
            last_hidden + avg_hidden + max_hidden + weighted_hidden
        ) / 4

        # å¤šä»»åŠ¡é¢„æµ‹
        outputs = {}
        for task_name, head in self.task_heads.items():
            outputs[task_name] = head(final_representation)

        return outputs

class IndustrialTrainingEngine:
    """å·¥ä¸šçº§è®­ç»ƒå¼•æ“"""
    def __init__(
        self,
        model: nn.Module,
        device_ids: List[int],
        mixed_precision: bool = True,
        gradient_checkpointing: bool = True
    ):
        self.model = model
        self.device_ids = device_ids
        self.mixed_precision = mixed_precision

        # åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®
        if len(device_ids) > 1:
            self.model = DDP(model, device_ids=device_ids)

        # æ··åˆç²¾åº¦è®­ç»ƒ
        if mixed_precision:
            self.scaler = torch.cuda.amp.GradScaler()

        # æ¢¯åº¦æ£€æŸ¥ç‚¹
        if gradient_checkpointing:
            self.model.gradient_checkpointing_enable()

    def train_epoch(self, dataloader, optimizer, scheduler, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        num_batches = len(dataloader)

        for batch_idx, (x, y, stock_ids) in enumerate(dataloader):
            x = x.cuda()
            stock_ids = stock_ids.cuda()

            # å¤šä»»åŠ¡æ ‡ç­¾
            y_direction = y['direction'].cuda()
            y_volatility = y['volatility'].cuda()
            y_return = y['return'].cuda()
            y_confidence = y['confidence'].cuda()
            y_risk = y['risk_level'].cuda()

            optimizer.zero_grad()

            if self.mixed_precision:
                with torch.cuda.amp.autocast():
                    outputs = self.model(x, stock_ids)

                    # å¤šä»»åŠ¡æŸå¤±
                    loss_direction = F.cross_entropy(outputs['direction'], y_direction)
                    loss_volatility = F.mse_loss(outputs['volatility'].squeeze(), y_volatility)
                    loss_return = F.mse_loss(outputs['return'].squeeze(), y_return)
                    loss_confidence = F.mse_loss(outputs['confidence'].squeeze(), y_confidence)
                    loss_risk = F.cross_entropy(outputs['risk_level'], y_risk)

                    # æ€»æŸå¤±
                    total_batch_loss = (
                        2.0 * loss_direction +      # æ–¹å‘é¢„æµ‹æƒé‡æœ€é«˜
                        1.5 * loss_return +         # æ”¶ç›Šç‡é¢„æµ‹
                        1.0 * loss_volatility +     # æ³¢åŠ¨ç‡é¢„æµ‹
                        1.0 * loss_confidence +     # ç½®ä¿¡åº¦é¢„æµ‹
                        0.5 * loss_risk             # é£é™©ç­‰çº§é¢„æµ‹
                    )

                self.scaler.scale(total_batch_loss).backward()

                # æ¢¯åº¦è£å‰ª
                self.scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                self.scaler.step(optimizer)
                self.scaler.update()
            else:
                outputs = self.model(x, stock_ids)

                # è®¡ç®—æŸå¤±ï¼ˆåŒä¸Šï¼‰
                loss_direction = F.cross_entropy(outputs['direction'], y_direction)
                loss_volatility = F.mse_loss(outputs['volatility'].squeeze(), y_volatility)
                loss_return = F.mse_loss(outputs['return'].squeeze(), y_return)
                loss_confidence = F.mse_loss(outputs['confidence'].squeeze(), y_confidence)
                loss_risk = F.cross_entropy(outputs['risk_level'], y_risk)

                total_batch_loss = (
                    2.0 * loss_direction + 1.5 * loss_return +
                    1.0 * loss_volatility + 1.0 * loss_confidence + 0.5 * loss_risk
                )

                total_batch_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                optimizer.step()

            scheduler.step()
            total_loss += total_batch_loss.item()

            # æ‰“å°è¿›åº¦
            if batch_idx % 100 == 0:
                print(f"Epoch {epoch}, Batch {batch_idx}/{num_batches}, "
                      f"Loss: {total_batch_loss.item():.6f}")

        return total_loss / num_batches

def create_industrial_model(config: Dict) -> IndustryLeadingTransformer:
    """åˆ›å»ºå·¥ä¸šçº§æ¨¡å‹"""
    return IndustryLeadingTransformer(
        num_stocks=config.get('num_stocks', 1000),
        raw_features=config.get('raw_features', 200),
        d_model=config.get('d_model', 1024),
        num_heads=config.get('num_heads', 16),
        num_layers=config.get('num_layers', 12),
        dim_feedforward=config.get('dim_feedforward', 4096),
        seq_len=config.get('seq_len', 252),
        dropout=config.get('dropout', 0.1),
        num_tasks=config.get('num_tasks', 5)
    )

def get_model_config() -> Dict:
    """è·å–æ¨¡å‹é…ç½®"""
    return {
        'num_stocks': 1000,      # 1000åªè‚¡ç¥¨
        'raw_features': 200,     # 200ä¸ªåŸå§‹ç‰¹å¾
        'd_model': 1024,         # æ¨¡å‹ç»´åº¦
        'num_heads': 16,         # æ³¨æ„åŠ›å¤´æ•°
        'num_layers': 12,        # Transformerå±‚æ•°
        'dim_feedforward': 4096, # å‰é¦ˆç½‘ç»œç»´åº¦
        'seq_len': 252,          # åºåˆ—é•¿åº¦ï¼ˆä¸€å¹´ï¼‰
        'dropout': 0.1,          # Dropoutç‡
        'num_tasks': 5           # å¤šä»»åŠ¡æ•°é‡
    }

if __name__ == "__main__":
    print("ğŸš€ å·¥ä¸šçº§é‡åŒ–äº¤æ˜“AIæ¨¡å‹æ¶æ„")
    print("=" * 60)

    config = get_model_config()
    model = create_industrial_model(config)

    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    print(f"   æ€»å‚æ•°é‡: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"   æ¨¡å‹å¤§å°: ~{total_params * 4 / 1024 / 1024:.1f} MB")
    print("=" * 60)

    # æ¨¡å‹æ¶æ„ä¿¡æ¯
    print(f"ğŸ—ï¸  æ¨¡å‹æ¶æ„:")
    print(f"   è‚¡ç¥¨æ•°é‡: {config['num_stocks']}")
    print(f"   ç‰¹å¾ç»´åº¦: {config['raw_features']}")
    print(f"   æ¨¡å‹ç»´åº¦: {config['d_model']}")
    print(f"   æ³¨æ„åŠ›å¤´: {config['num_heads']}")
    print(f"   å±‚æ•°: {config['num_layers']}")
    print(f"   åºåˆ—é•¿åº¦: {config['seq_len']}")
    print(f"   å¤šä»»åŠ¡æ•°: {config['num_tasks']}")
    print("=" * 60)
    print("âœ… æ¨¡å‹æ¶æ„è®¾è®¡å®Œæˆ!")