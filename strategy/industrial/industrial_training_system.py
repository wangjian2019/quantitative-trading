#!/usr/bin/env python3
"""
å·¥ä¸šçº§GPUé›†ç¾¤è®­ç»ƒç³»ç»Ÿ
ä¸“ä¸ºå¤§è§„æ¨¡é‡åŒ–äº¤æ˜“AIè®¾è®¡çš„åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶

ä½œè€…: Alvin
ç‰¹æ€§:
- å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ
- æ··åˆç²¾åº¦è®­ç»ƒ
- æ¢¯åº¦ç´¯ç§¯å’Œæ£€æŸ¥ç‚¹
- è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦
- æ¨¡å‹å¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œ
- å®æ—¶ç›‘æ§å’Œå¯è§†åŒ–
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler, Dataset
from torch.cuda.amp import GradScaler, autocast

import numpy as np
import pandas as pd
import os
import json
import logging
import time
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
import multiprocessing as mp
from pathlib import Path
import warnings
warnings.filterwarnings("ignore")

# å¯¼å…¥å·¥ä¸šçº§æ¨¡å‹
from industrial_scale_model import IndustryLeadingTransformer, get_model_config

class IndustrialDataset(Dataset):
    """å·¥ä¸šçº§æ•°æ®é›†ç±»"""

    def __init__(
        self,
        data_path: str,
        sequence_length: int = 252,
        prediction_horizon: int = 5,
        symbols: Optional[List[str]] = None
    ):
        self.data_path = Path(data_path)
        self.sequence_length = sequence_length
        self.prediction_horizon = prediction_horizon

        # åŠ è½½æ‰€æœ‰æ•°æ®æ–‡ä»¶
        self.data_files = list(self.data_path.glob("*.parquet"))
        if symbols:
            # è¿‡æ»¤æŒ‡å®šè‚¡ç¥¨
            symbol_files = [f for f in self.data_files
                          if any(s in f.stem for s in symbols)]
            self.data_files = symbol_files

        self.samples = self._prepare_samples()
        self.symbol_to_id = self._create_symbol_mapping()

        print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        print(f"   æ•°æ®æ–‡ä»¶: {len(self.data_files)}")
        print(f"   æ€»æ ·æœ¬æ•°: {len(self.samples)}")
        print(f"   åºåˆ—é•¿åº¦: {self.sequence_length}")
        print(f"   é¢„æµ‹è·¨åº¦: {self.prediction_horizon}")

    def _create_symbol_mapping(self) -> Dict[str, int]:
        """åˆ›å»ºè‚¡ç¥¨ç¬¦å·åˆ°IDçš„æ˜ å°„"""
        symbols = list(set(sample[2] for sample in self.samples))
        return {symbol: idx for idx, symbol in enumerate(sorted(symbols))}

    def _prepare_samples(self) -> List[Tuple[str, int, str]]:
        """å‡†å¤‡è®­ç»ƒæ ·æœ¬ç´¢å¼•"""
        samples = []

        for file_path in self.data_files:
            try:
                # è¯»å–æ•°æ®
                df = pd.read_parquet(file_path)

                if len(df) < self.sequence_length + self.prediction_horizon:
                    continue

                symbol = df['symbol'].iloc[0] if 'symbol' in df.columns else file_path.stem

                # ä¸ºæ¯ä¸ªå¯èƒ½çš„åºåˆ—åˆ›å»ºæ ·æœ¬
                for start_idx in range(len(df) - self.sequence_length - self.prediction_horizon + 1):
                    samples.append((str(file_path), start_idx, symbol))

            except Exception as e:
                print(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥ {file_path}: {e}")
                continue

        return samples

    def _calculate_targets(self, data: pd.DataFrame, start_idx: int) -> Dict[str, torch.Tensor]:
        """è®¡ç®—å¤šä»»åŠ¡å­¦ä¹ ç›®æ ‡"""
        current_idx = start_idx + self.sequence_length - 1
        future_idx = min(current_idx + self.prediction_horizon, len(data) - 1)

        current_price = data['close'].iloc[current_idx]
        future_price = data['close'].iloc[future_idx]

        # 1. æ–¹å‘é¢„æµ‹ (æ¶¨/è·Œ/æ¨ªç›˜)
        price_change = (future_price - current_price) / current_price
        if price_change > 0.02:  # æ¶¨å¹…è¶…è¿‡2%
            direction = 2  # BUY
        elif price_change < -0.02:  # è·Œå¹…è¶…è¿‡2%
            direction = 0  # SELL
        else:
            direction = 1  # HOLD

        # 2. æ”¶ç›Šç‡é¢„æµ‹
        return_rate = float(price_change)

        # 3. æ³¢åŠ¨ç‡é¢„æµ‹
        price_series = data['close'].iloc[current_idx-min(20, current_idx):current_idx+1]
        volatility = float(price_series.pct_change().std() * np.sqrt(252))

        # 4. ç½®ä¿¡åº¦ (åŸºäºå†å²æ³¢åŠ¨ç‡çš„å€’æ•°)
        confidence = float(1.0 / (1.0 + volatility)) if volatility > 0 else 0.5

        # 5. é£é™©ç­‰çº§ (åŸºäºæ³¢åŠ¨ç‡åˆ†ä½æ•°)
        if volatility < 0.1:
            risk_level = 0  # ä½é£é™©
        elif volatility < 0.2:
            risk_level = 1  # ä¸­ä½é£é™©
        elif volatility < 0.3:
            risk_level = 2  # ä¸­ç­‰é£é™©
        elif volatility < 0.5:
            risk_level = 3  # ä¸­é«˜é£é™©
        else:
            risk_level = 4  # é«˜é£é™©

        return {
            'direction': torch.tensor(direction, dtype=torch.long),
            'return': torch.tensor(return_rate, dtype=torch.float32),
            'volatility': torch.tensor(volatility, dtype=torch.float32),
            'confidence': torch.tensor(confidence, dtype=torch.float32),
            'risk_level': torch.tensor(risk_level, dtype=torch.long)
        }

    def __len__(self) -> int:
        return len(self.samples)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor]:
        file_path, start_idx, symbol = self.samples[idx]

        # è¯»å–æ•°æ®
        df = pd.read_parquet(file_path)

        # æå–ç‰¹å¾åºåˆ—
        end_idx = start_idx + self.sequence_length
        sequence_data = df.iloc[start_idx:end_idx]

        # é€‰æ‹©ç‰¹å¾åˆ— (æ’é™¤éæ•°å€¼åˆ—)
        feature_columns = [col for col in sequence_data.columns
                          if col not in ['symbol'] and
                          pd.api.types.is_numeric_dtype(sequence_data[col])]

        features = sequence_data[feature_columns].values.astype(np.float32)

        # å¤„ç†NaNå€¼
        features = np.nan_to_num(features, nan=0.0, posinf=1e6, neginf=-1e6)

        # è®¡ç®—ç›®æ ‡
        targets = self._calculate_targets(df, start_idx)

        # è·å–è‚¡ç¥¨ID
        stock_id = torch.tensor(self.symbol_to_id[symbol], dtype=torch.long)

        return torch.tensor(features), targets, stock_id

class IndustrialTrainer:
    """å·¥ä¸šçº§è®­ç»ƒå™¨"""

    def __init__(
        self,
        model: nn.Module,
        train_dataset: Dataset,
        val_dataset: Dataset,
        config: Dict[str, Any],
        device_ids: List[int] = None
    ):
        self.config = config
        self.device_ids = device_ids or [0]
        self.device = torch.device(f"cuda:{self.device_ids[0]}" if torch.cuda.is_available() else "cpu")

        print(f"ğŸš€ åˆå§‹åŒ–å·¥ä¸šçº§è®­ç»ƒå™¨...")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   GPUæ•°é‡: {len(self.device_ids)}")

        # æ¨¡å‹è®¾ç½®
        self.model = model.to(self.device)
        if len(self.device_ids) > 1:
            self.model = DDP(self.model, device_ids=self.device_ids)

        # æ•°æ®åŠ è½½å™¨
        self.train_loader = self._create_dataloader(train_dataset, shuffle=True)
        self.val_loader = self._create_dataloader(val_dataset, shuffle=False)

        # ä¼˜åŒ–å™¨è®¾ç½®
        self.optimizer = self._setup_optimizer()
        self.scheduler = self._setup_scheduler()

        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.use_amp = config.get('mixed_precision', True)
        if self.use_amp:
            self.scaler = GradScaler()

        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        self.training_history = []

        # æ¢¯åº¦ç´¯ç§¯
        self.accumulation_steps = config.get('gradient_accumulation_steps', 4)

        print(f"âœ… è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")

    def _create_dataloader(self, dataset: Dataset, shuffle: bool = True) -> DataLoader:
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        sampler = None
        if len(self.device_ids) > 1:
            sampler = DistributedSampler(dataset, shuffle=shuffle)
            shuffle = False  # ä½¿ç”¨sampleræ—¶ä¸èƒ½shuffle

        return DataLoader(
            dataset,
            batch_size=self.config['batch_size'],
            shuffle=shuffle,
            sampler=sampler,
            num_workers=self.config.get('num_workers', 8),
            pin_memory=True,
            drop_last=True
        )

    def _setup_optimizer(self) -> torch.optim.Optimizer:
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        optimizer_config = self.config.get('optimizer', {})
        optimizer_type = optimizer_config.get('type', 'AdamW')

        if optimizer_type == 'AdamW':
            return torch.optim.AdamW(
                self.model.parameters(),
                lr=optimizer_config.get('lr', 1e-4),
                weight_decay=optimizer_config.get('weight_decay', 0.01),
                betas=optimizer_config.get('betas', (0.9, 0.999))
            )
        elif optimizer_type == 'Adam':
            return torch.optim.Adam(
                self.model.parameters(),
                lr=optimizer_config.get('lr', 1e-4),
                weight_decay=optimizer_config.get('weight_decay', 0.01)
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹: {optimizer_type}")

    def _setup_scheduler(self) -> torch.optim.lr_scheduler._LRScheduler:
        """è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        scheduler_config = self.config.get('scheduler', {})
        scheduler_type = scheduler_config.get('type', 'CosineAnnealingLR')

        if scheduler_type == 'CosineAnnealingLR':
            return torch.optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config['epochs'],
                eta_min=scheduler_config.get('eta_min', 1e-6)
            )
        elif scheduler_type == 'StepLR':
            return torch.optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=scheduler_config.get('step_size', 30),
                gamma=scheduler_config.get('gamma', 0.1)
            )
        elif scheduler_type == 'ReduceLROnPlateau':
            return torch.optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                factor=scheduler_config.get('factor', 0.5),
                patience=scheduler_config.get('patience', 10)
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è°ƒåº¦å™¨ç±»å‹: {scheduler_type}")

    def _calculate_loss(self, outputs: Dict[str, torch.Tensor], targets: Dict[str, torch.Tensor]) -> torch.Tensor:
        """è®¡ç®—å¤šä»»åŠ¡æŸå¤±"""
        losses = {}

        # æ–¹å‘é¢„æµ‹æŸå¤±
        if 'direction' in outputs and 'direction' in targets:
            losses['direction'] = F.cross_entropy(outputs['direction'], targets['direction'])

        # æ”¶ç›Šç‡é¢„æµ‹æŸå¤±
        if 'return' in outputs and 'return' in targets:
            losses['return'] = F.mse_loss(outputs['return'].squeeze(), targets['return'])

        # æ³¢åŠ¨ç‡é¢„æµ‹æŸå¤±
        if 'volatility' in outputs and 'volatility' in targets:
            losses['volatility'] = F.mse_loss(outputs['volatility'].squeeze(), targets['volatility'])

        # ç½®ä¿¡åº¦é¢„æµ‹æŸå¤±
        if 'confidence' in outputs and 'confidence' in targets:
            losses['confidence'] = F.mse_loss(outputs['confidence'].squeeze(), targets['confidence'])

        # é£é™©ç­‰çº§é¢„æµ‹æŸå¤±
        if 'risk_level' in outputs and 'risk_level' in targets:
            losses['risk_level'] = F.cross_entropy(outputs['risk_level'], targets['risk_level'])

        # åŠ æƒæ€»æŸå¤±
        loss_weights = self.config.get('loss_weights', {
            'direction': 2.0,    # æ–¹å‘é¢„æµ‹æœ€é‡è¦
            'return': 1.5,       # æ”¶ç›Šç‡é¢„æµ‹
            'volatility': 1.0,   # æ³¢åŠ¨ç‡é¢„æµ‹
            'confidence': 1.0,   # ç½®ä¿¡åº¦é¢„æµ‹
            'risk_level': 0.5    # é£é™©ç­‰çº§é¢„æµ‹
        })

        total_loss = sum(
            loss_weights.get(task, 1.0) * loss
            for task, loss in losses.items()
        )

        return total_loss, losses

    def train_epoch(self) -> Tuple[float, Dict[str, float]]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        task_losses = {}
        num_batches = len(self.train_loader)

        for batch_idx, (features, targets, stock_ids) in enumerate(self.train_loader):
            # æ•°æ®ç§»åŠ¨åˆ°GPU
            features = features.to(self.device)
            stock_ids = stock_ids.to(self.device)

            batch_targets = {}
            for task, target in targets.items():
                batch_targets[task] = target.to(self.device)

            # å‰å‘ä¼ æ’­
            if self.use_amp:
                with autocast():
                    outputs = self.model(features, stock_ids)
                    loss, losses = self._calculate_loss(outputs, batch_targets)
                    loss = loss / self.accumulation_steps

                # åå‘ä¼ æ’­
                self.scaler.scale(loss).backward()

                # æ¢¯åº¦ç´¯ç§¯
                if (batch_idx + 1) % self.accumulation_steps == 0:
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                    self.optimizer.zero_grad()
            else:
                outputs = self.model(features, stock_ids)
                loss, losses = self._calculate_loss(outputs, batch_targets)
                loss = loss / self.accumulation_steps

                loss.backward()

                if (batch_idx + 1) % self.accumulation_steps == 0:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    self.optimizer.step()
                    self.optimizer.zero_grad()

            # ç»Ÿè®¡æŸå¤±
            total_loss += loss.item() * self.accumulation_steps
            for task, task_loss in losses.items():
                if task not in task_losses:
                    task_losses[task] = 0.0
                task_losses[task] += task_loss.item()

            # è¿›åº¦æŠ¥å‘Š
            if batch_idx % 100 == 0:
                current_lr = self.optimizer.param_groups[0]['lr']
                print(f"   Batch {batch_idx}/{num_batches} | "
                      f"Loss: {loss.item():.6f} | "
                      f"LR: {current_lr:.2e}")

        # å¹³å‡æŸå¤±
        avg_loss = total_loss / num_batches
        avg_task_losses = {task: loss / num_batches for task, loss in task_losses.items()}

        return avg_loss, avg_task_losses

    def validate_epoch(self) -> Tuple[float, Dict[str, float]]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        task_losses = {}
        num_batches = len(self.val_loader)

        with torch.no_grad():
            for features, targets, stock_ids in self.val_loader:
                # æ•°æ®ç§»åŠ¨åˆ°GPU
                features = features.to(self.device)
                stock_ids = stock_ids.to(self.device)

                batch_targets = {}
                for task, target in targets.items():
                    batch_targets[task] = target.to(self.device)

                # å‰å‘ä¼ æ’­
                if self.use_amp:
                    with autocast():
                        outputs = self.model(features, stock_ids)
                        loss, losses = self._calculate_loss(outputs, batch_targets)
                else:
                    outputs = self.model(features, stock_ids)
                    loss, losses = self._calculate_loss(outputs, batch_targets)

                # ç»Ÿè®¡æŸå¤±
                total_loss += loss.item()
                for task, task_loss in losses.items():
                    if task not in task_losses:
                        task_losses[task] = 0.0
                    task_losses[task] += task_loss.item()

        # å¹³å‡æŸå¤±
        avg_loss = total_loss / num_batches
        avg_task_losses = {task: loss / num_batches for task, loss in task_losses.items()}

        return avg_loss, avg_task_losses

    def train(self) -> Dict[str, List[float]]:
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"ğŸš€ å¼€å§‹å·¥ä¸šçº§è®­ç»ƒ...")
        print(f"   è®­ç»ƒæ ·æœ¬: {len(self.train_loader.dataset)}")
        print(f"   éªŒè¯æ ·æœ¬: {len(self.val_loader.dataset)}")
        print(f"   æ‰¹å¤§å°: {self.config['batch_size']}")
        print(f"   è®­ç»ƒè½®æ•°: {self.config['epochs']}")
        print("=" * 80)

        history = {
            'train_loss': [],
            'val_loss': [],
            'learning_rate': []
        }

        for epoch in range(self.config['epochs']):
            self.current_epoch = epoch
            epoch_start_time = time.time()

            print(f"ğŸ”„ Epoch {epoch + 1}/{self.config['epochs']}")

            # è®­ç»ƒ
            train_loss, train_task_losses = self.train_epoch()
            history['train_loss'].append(train_loss)

            # éªŒè¯
            val_loss, val_task_losses = self.validate_epoch()
            history['val_loss'].append(val_loss)

            # å­¦ä¹ ç‡è°ƒåº¦
            if isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                self.scheduler.step(val_loss)
            else:
                self.scheduler.step()

            current_lr = self.optimizer.param_groups[0]['lr']
            history['learning_rate'].append(current_lr)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.save_checkpoint(f"best_model_epoch_{epoch+1}.pth")

            # æ¯10ä¸ªepochä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % 10 == 0:
                self.save_checkpoint(f"checkpoint_epoch_{epoch+1}.pth")

            epoch_time = time.time() - epoch_start_time

            # æ‰“å°ç»“æœ
            print(f"   è®­ç»ƒæŸå¤±: {train_loss:.6f}")
            print(f"   éªŒè¯æŸå¤±: {val_loss:.6f}")
            print(f"   å­¦ä¹ ç‡: {current_lr:.2e}")
            print(f"   è€—æ—¶: {epoch_time:.2f}s")

            # æ‰“å°ä»»åŠ¡æŸå¤±
            if train_task_losses:
                print("   ä»»åŠ¡æŸå¤±:")
                for task, loss in train_task_losses.items():
                    print(f"     {task}: {loss:.6f}")

            print("-" * 50)

        print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
        return history

    def save_checkpoint(self, filename: str):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.current_epoch,
            'model_state_dict': self.model.state_dict() if not isinstance(self.model, DDP) else self.model.module.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'config': self.config
        }

        if self.use_amp:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()

        torch.save(checkpoint, filename)
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filename}")

def setup_distributed_training(rank: int, world_size: int):
    """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ"""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'

    # åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

def cleanup_distributed_training():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒ"""
    dist.destroy_process_group()

def get_training_config() -> Dict[str, Any]:
    """è·å–è®­ç»ƒé…ç½®"""
    return {
        # åŸºæœ¬è®¾ç½®
        'batch_size': 32,
        'epochs': 100,
        'learning_rate': 1e-4,
        'mixed_precision': True,
        'gradient_accumulation_steps': 4,

        # æ•°æ®è®¾ç½®
        'num_workers': 8,
        'sequence_length': 252,
        'prediction_horizon': 5,

        # ä¼˜åŒ–å™¨è®¾ç½®
        'optimizer': {
            'type': 'AdamW',
            'lr': 1e-4,
            'weight_decay': 0.01,
            'betas': (0.9, 0.999)
        },

        # è°ƒåº¦å™¨è®¾ç½®
        'scheduler': {
            'type': 'CosineAnnealingLR',
            'eta_min': 1e-6
        },

        # æŸå¤±æƒé‡
        'loss_weights': {
            'direction': 2.0,
            'return': 1.5,
            'volatility': 1.0,
            'confidence': 1.0,
            'risk_level': 0.5
        }
    }

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("ğŸš€ å·¥ä¸šçº§GPUé›†ç¾¤è®­ç»ƒç³»ç»Ÿ")
    print("=" * 80)

    # æ£€æŸ¥GPU
    if not torch.cuda.is_available():
        print("âŒ æœªæ£€æµ‹åˆ°CUDAæ”¯æŒ")
        return

    gpu_count = torch.cuda.device_count()
    print(f"ğŸ¯ æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")

    # é…ç½®
    model_config = get_model_config()
    training_config = get_training_config()

    print("ğŸ“Š é…ç½®ä¿¡æ¯:")
    print(f"   æ¨¡å‹å‚æ•°: {sum(p.numel() for p in IndustryLeadingTransformer(**model_config).parameters()):,}")
    print(f"   æ‰¹å¤§å°: {training_config['batch_size']}")
    print(f"   è®­ç»ƒè½®æ•°: {training_config['epochs']}")
    print(f"   åºåˆ—é•¿åº¦: {training_config['sequence_length']}")
    print("=" * 80)

    # åˆ›å»ºæ¨¡å‹
    model = IndustryLeadingTransformer(**model_config)

    # åˆ›å»ºæ•°æ®é›†
    data_path = "data/industrial_training_data"
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {data_path}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ industrial_data_collector.py æ”¶é›†æ•°æ®")
        return

    print("ğŸ“¦ åŠ è½½æ•°æ®é›†...")
    full_dataset = IndustrialDataset(
        data_path=data_path,
        sequence_length=training_config['sequence_length'],
        prediction_horizon=training_config['prediction_horizon']
    )

    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        full_dataset, [train_size, val_size]
    )

    print(f"ğŸ“Š æ•°æ®é›†åˆ’åˆ†:")
    print(f"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"   éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")

    # è®¾ç½®è®¾å¤‡
    device_ids = list(range(gpu_count))

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = IndustrialTrainer(
        model=model,
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        config=training_config,
        device_ids=device_ids
    )

    # å¼€å§‹è®­ç»ƒ
    history = trainer.train()

    # ä¿å­˜è®­ç»ƒå†å²
    with open("training_history.json", 'w') as f:
        json.dump(history, f, indent=2)

    print("âœ… è®­ç»ƒç³»ç»Ÿè¿è¡Œå®Œæ¯•!")

if __name__ == "__main__":
    main()