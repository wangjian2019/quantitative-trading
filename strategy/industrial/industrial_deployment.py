#!/usr/bin/env python3
"""
å·¥ä¸šçº§æ¨¡å‹éƒ¨ç½²ç³»ç»Ÿ
é«˜æ€§èƒ½é‡åŒ–äº¤æ˜“AIæ¨ç†æœåŠ¡

ä½œè€…: Alvin
ç‰¹æ€§:
- å¤šGPUæ¨ç†åŠ é€Ÿ
- æ¨¡å‹å¹¶è¡Œéƒ¨ç½²
- å®æ—¶æ‰¹å¤„ç†æ¨ç†
- è‡ªåŠ¨æ¨¡å‹çƒ­æ›´æ–°
- é«˜å¯ç”¨æœåŠ¡æ¶æ„
- æ€§èƒ½ç›‘æ§å’Œè´Ÿè½½å‡è¡¡
"""

import torch
import torch.nn as nn
from torch.cuda.amp import autocast
import numpy as np
import pandas as pd
import asyncio
import aiohttp
from aiohttp import web
import uvloop
import logging
from typing import Dict, List, Optional, Any, Tuple
import json
import time
from datetime import datetime, timedelta
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import queue
import threading
from pathlib import Path
import psutil
import warnings
warnings.filterwarnings("ignore")

# å¯¼å…¥æ¨¡å‹
from industrial_scale_model import IndustryLeadingTransformer, get_model_config

class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨"""

    def __init__(self, model_path: str, config: Dict[str, Any]):
        self.model_path = model_path
        self.config = config
        self.models = {}  # GPU_ID -> model
        self.device_count = torch.cuda.device_count()
        self.current_model_version = None

        print(f"ğŸš€ åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨")
        print(f"   GPUæ•°é‡: {self.device_count}")
        print(f"   æ¨¡å‹è·¯å¾„: {model_path}")

        self.load_models()

    def load_models(self):
        """åœ¨æ‰€æœ‰GPUä¸ŠåŠ è½½æ¨¡å‹"""
        if not torch.cuda.is_available():
            print("âŒ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæ¨¡å¼")
            device = torch.device('cpu')
            model = IndustryLeadingTransformer(**self.config)
            model.load_state_dict(torch.load(self.model_path, map_location=device)['model_state_dict'])
            model.eval()
            self.models[0] = model
            return

        # åœ¨æ¯ä¸ªGPUä¸ŠåŠ è½½æ¨¡å‹å‰¯æœ¬
        for gpu_id in range(self.device_count):
            device = torch.device(f'cuda:{gpu_id}')
            model = IndustryLeadingTransformer(**self.config)

            # åŠ è½½æ¨¡å‹æƒé‡
            checkpoint = torch.load(self.model_path, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            model.to(device)
            model.eval()

            self.models[gpu_id] = model
            print(f"âœ… GPU {gpu_id} æ¨¡å‹åŠ è½½å®Œæˆ")

        self.current_model_version = datetime.now().isoformat()

    def get_model(self, gpu_id: Optional[int] = None) -> Tuple[nn.Module, torch.device]:
        """è·å–æ¨¡å‹å®ä¾‹"""
        if gpu_id is None:
            # è‡ªåŠ¨é€‰æ‹©è´Ÿè½½æœ€ä½çš„GPU
            gpu_id = self._select_optimal_gpu()

        if gpu_id not in self.models:
            gpu_id = 0

        device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')
        return self.models[gpu_id], device

    def _select_optimal_gpu(self) -> int:
        """é€‰æ‹©æœ€ä¼˜GPU"""
        if not torch.cuda.is_available():
            return 0

        # ç®€å•è½®è¯¢ç­–ç•¥
        return int(time.time()) % self.device_count

    def update_model(self, new_model_path: str):
        """çƒ­æ›´æ–°æ¨¡å‹"""
        print(f"ğŸ”„ å¼€å§‹æ¨¡å‹çƒ­æ›´æ–°: {new_model_path}")
        try:
            # åˆ›å»ºæ–°æ¨¡å‹å®ä¾‹
            new_models = {}

            for gpu_id in range(self.device_count if torch.cuda.is_available() else 1):
                device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')
                model = IndustryLeadingTransformer(**self.config)

                checkpoint = torch.load(new_model_path, map_location=device)
                model.load_state_dict(checkpoint['model_state_dict'])
                model.to(device)
                model.eval()

                new_models[gpu_id] = model

            # åŸå­æ›¿æ¢
            old_models = self.models
            self.models = new_models
            self.model_path = new_model_path
            self.current_model_version = datetime.now().isoformat()

            print(f"âœ… æ¨¡å‹çƒ­æ›´æ–°å®Œæˆ")

            # æ¸…ç†æ—§æ¨¡å‹
            del old_models

        except Exception as e:
            print(f"âŒ æ¨¡å‹æ›´æ–°å¤±è´¥: {e}")

class PredictionBatch:
    """é¢„æµ‹æ‰¹æ¬¡"""

    def __init__(self, batch_id: str, requests: List[Dict]):
        self.batch_id = batch_id
        self.requests = requests
        self.created_at = time.time()
        self.results = {}
        self.completed = False

class IndustrialInferenceEngine:
    """å·¥ä¸šçº§æ¨ç†å¼•æ“"""

    def __init__(self, model_manager: ModelManager, config: Dict[str, Any]):
        self.model_manager = model_manager
        self.config = config

        # æ‰¹å¤„ç†è®¾ç½®
        self.batch_size = config.get('batch_size', 64)
        self.batch_timeout = config.get('batch_timeout', 0.1)  # 100ms
        self.max_sequence_length = config.get('max_sequence_length', 252)

        # é¢„å¤„ç†é˜Ÿåˆ—
        self.request_queue = queue.Queue()
        self.batch_queue = queue.Queue()
        self.result_cache = {}

        # çº¿ç¨‹æ± 
        self.executor = ThreadPoolExecutor(max_workers=config.get('inference_workers', 4))

        # å¯åŠ¨æ‰¹å¤„ç†çº¿ç¨‹
        self.batch_thread = threading.Thread(target=self._batch_processor, daemon=True)
        self.batch_thread.start()

        # å¯åŠ¨æ¨ç†çº¿ç¨‹
        for i in range(config.get('inference_workers', 4)):
            worker_thread = threading.Thread(target=self._inference_worker, daemon=True)
            worker_thread.start()

        print(f"ğŸš€ æ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ‰¹å¤§å°: {self.batch_size}")
        print(f"   æ‰¹è¶…æ—¶: {self.batch_timeout}s")
        print(f"   æ¨ç†å·¥ä½œçº¿ç¨‹: {config.get('inference_workers', 4)}")

    def _batch_processor(self):
        """æ‰¹å¤„ç†å™¨"""
        current_batch = []
        last_batch_time = time.time()

        while True:
            try:
                # å°è¯•è·å–è¯·æ±‚
                try:
                    request = self.request_queue.get(timeout=0.01)
                    current_batch.append(request)
                except queue.Empty:
                    request = None

                # æ£€æŸ¥æ˜¯å¦éœ€è¦å‘é€æ‰¹æ¬¡
                should_send_batch = (
                    len(current_batch) >= self.batch_size or
                    (current_batch and time.time() - last_batch_time > self.batch_timeout)
                )

                if should_send_batch and current_batch:
                    # åˆ›å»ºæ‰¹æ¬¡
                    batch_id = f"batch_{int(time.time() * 1000)}"
                    batch = PredictionBatch(batch_id, current_batch.copy())

                    # å‘é€åˆ°æ¨ç†é˜Ÿåˆ—
                    self.batch_queue.put(batch)

                    # é‡ç½®
                    current_batch.clear()
                    last_batch_time = time.time()

            except Exception as e:
                print(f"âŒ æ‰¹å¤„ç†å™¨é”™è¯¯: {e}")

    def _inference_worker(self):
        """æ¨ç†å·¥ä½œçº¿ç¨‹"""
        while True:
            try:
                # è·å–æ‰¹æ¬¡
                batch = self.batch_queue.get()

                # æ‰§è¡Œæ¨ç†
                self._process_batch(batch)

                # æ ‡è®°å®Œæˆ
                self.batch_queue.task_done()

            except Exception as e:
                print(f"âŒ æ¨ç†å·¥ä½œçº¿ç¨‹é”™è¯¯: {e}")

    def _process_batch(self, batch: PredictionBatch):
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
        try:
            start_time = time.time()

            # è·å–æ¨¡å‹
            model, device = self.model_manager.get_model()

            # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
            batch_features = []
            batch_stock_ids = []
            valid_indices = []

            for i, request in enumerate(batch.requests):
                try:
                    features, stock_id = self._prepare_request_data(request)
                    batch_features.append(features)
                    batch_stock_ids.append(stock_id)
                    valid_indices.append(i)
                except Exception as e:
                    # æ ‡è®°å¤±è´¥è¯·æ±‚
                    batch.results[request['request_id']] = {
                        'error': str(e),
                        'timestamp': datetime.now().isoformat()
                    }

            if not batch_features:
                return

            # è½¬æ¢ä¸ºå¼ é‡
            batch_tensor = torch.stack(batch_features).to(device)
            stock_ids_tensor = torch.tensor(batch_stock_ids, dtype=torch.long).to(device)

            # æ¨ç†
            with torch.no_grad():
                if self.config.get('use_amp', True):
                    with autocast():
                        outputs = model(batch_tensor, stock_ids_tensor)
                else:
                    outputs = model(batch_tensor, stock_ids_tensor)

            # å¤„ç†ç»“æœ
            for i, request_idx in enumerate(valid_indices):
                request = batch.requests[request_idx]
                result = self._process_model_output(outputs, i, request)
                batch.results[request['request_id']] = result

            inference_time = time.time() - start_time
            print(f"âš¡ æ‰¹æ¬¡ {batch.batch_id} å®Œæˆ: {len(valid_indices)} ä¸ªè¯·æ±‚, "
                  f"è€—æ—¶ {inference_time:.3f}s")

            # ç¼“å­˜ç»“æœ
            for request_id, result in batch.results.items():
                self.result_cache[request_id] = result

            batch.completed = True

        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥ {batch.batch_id}: {e}")

            # æ ‡è®°æ‰€æœ‰è¯·æ±‚å¤±è´¥
            for request in batch.requests:
                batch.results[request['request_id']] = {
                    'error': f"æ‰¹æ¬¡å¤„ç†å¤±è´¥: {str(e)}",
                    'timestamp': datetime.now().isoformat()
                }

    def _prepare_request_data(self, request: Dict) -> Tuple[torch.Tensor, int]:
        """å‡†å¤‡è¯·æ±‚æ•°æ®"""
        # æå–ç‰¹å¾
        features = np.array(request['features'], dtype=np.float32)

        # æ£€æŸ¥å½¢çŠ¶
        if features.shape[0] > self.max_sequence_length:
            features = features[-self.max_sequence_length:]
        elif features.shape[0] < self.max_sequence_length:
            # å¡«å……
            padding = np.zeros((self.max_sequence_length - features.shape[0], features.shape[1]), dtype=np.float32)
            features = np.vstack([padding, features])

        # å¤„ç†NaN
        features = np.nan_to_num(features, nan=0.0, posinf=1e6, neginf=-1e6)

        # è‚¡ç¥¨ID
        stock_id = request.get('stock_id', 0)

        return torch.tensor(features), stock_id

    def _process_model_output(self, outputs: Dict[str, torch.Tensor], index: int, request: Dict) -> Dict:
        """å¤„ç†æ¨¡å‹è¾“å‡º"""
        result = {
            'timestamp': datetime.now().isoformat(),
            'model_version': self.model_manager.current_model_version,
            'symbol': request.get('symbol', 'UNKNOWN')
        }

        # æ–¹å‘é¢„æµ‹
        if 'direction' in outputs:
            direction_logits = outputs['direction'][index]
            direction_probs = torch.softmax(direction_logits, dim=0)
            predicted_direction = torch.argmax(direction_probs).item()

            direction_names = ['SELL', 'HOLD', 'BUY']
            result['action'] = direction_names[predicted_direction]
            result['confidence'] = float(torch.max(direction_probs))

        # æ”¶ç›Šç‡é¢„æµ‹
        if 'return' in outputs:
            result['expected_return'] = float(outputs['return'][index])

        # æ³¢åŠ¨ç‡é¢„æµ‹
        if 'volatility' in outputs:
            result['volatility'] = float(outputs['volatility'][index])

        # ç½®ä¿¡åº¦ï¼ˆå¦‚æœæ¨¡å‹è¾“å‡ºï¼‰
        if 'confidence' in outputs:
            result['model_confidence'] = float(outputs['confidence'][index])

        # é£é™©ç­‰çº§
        if 'risk_level' in outputs:
            risk_logits = outputs['risk_level'][index]
            risk_probs = torch.softmax(risk_logits, dim=0)
            predicted_risk = torch.argmax(risk_probs).item()

            risk_names = ['LOW', 'MEDIUM_LOW', 'MEDIUM', 'MEDIUM_HIGH', 'HIGH']
            result['risk_level'] = risk_names[predicted_risk]
            result['risk_confidence'] = float(torch.max(risk_probs))

        return result

    async def predict(self, request: Dict) -> Dict:
        """å¼‚æ­¥é¢„æµ‹æ¥å£"""
        request_id = f"req_{int(time.time() * 1000000)}"
        request['request_id'] = request_id

        # åŠ å…¥é˜Ÿåˆ—
        self.request_queue.put(request)

        # è½®è¯¢ç»“æœ
        max_wait_time = 10.0  # æœ€å¤§ç­‰å¾…æ—¶é—´
        start_time = time.time()

        while time.time() - start_time < max_wait_time:
            if request_id in self.result_cache:
                result = self.result_cache.pop(request_id)
                return result

            await asyncio.sleep(0.01)  # 10ms

        # è¶…æ—¶
        return {
            'error': 'Request timeout',
            'timestamp': datetime.now().isoformat()
        }

class IndustrialAPIServer:
    """å·¥ä¸šçº§APIæœåŠ¡å™¨"""

    def __init__(self, inference_engine: IndustrialInferenceEngine):
        self.inference_engine = inference_engine
        self.app = web.Application()
        self.setup_routes()

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'average_response_time': 0.0,
            'start_time': time.time()
        }

    def setup_routes(self):
        """è®¾ç½®è·¯ç”±"""
        self.app.router.add_post('/predict', self.predict_handler)
        self.app.router.add_post('/batch_predict', self.batch_predict_handler)
        self.app.router.add_get('/health', self.health_handler)
        self.app.router.add_get('/stats', self.stats_handler)
        self.app.router.add_get('/model_info', self.model_info_handler)

    async def predict_handler(self, request):
        """å•ä¸ªé¢„æµ‹è¯·æ±‚å¤„ç†"""
        start_time = time.time()
        self.stats['total_requests'] += 1

        try:
            data = await request.json()

            # éªŒè¯è¾“å…¥
            if 'features' not in data:
                raise ValueError("ç¼ºå°‘ 'features' å­—æ®µ")

            # æ‰§è¡Œé¢„æµ‹
            result = await self.inference_engine.predict(data)

            # æ›´æ–°ç»Ÿè®¡
            response_time = time.time() - start_time
            self.stats['successful_requests'] += 1
            self._update_average_response_time(response_time)

            return web.json_response(result)

        except Exception as e:
            self.stats['failed_requests'] += 1
            return web.json_response(
                {'error': str(e), 'timestamp': datetime.now().isoformat()},
                status=400
            )

    async def batch_predict_handler(self, request):
        """æ‰¹é‡é¢„æµ‹è¯·æ±‚å¤„ç†"""
        start_time = time.time()

        try:
            data = await request.json()
            requests = data.get('requests', [])

            if not requests:
                raise ValueError("è¯·æ±‚åˆ—è¡¨ä¸èƒ½ä¸ºç©º")

            # å¹¶è¡Œå¤„ç†æ‰€æœ‰è¯·æ±‚
            tasks = [
                self.inference_engine.predict(req)
                for req in requests
            ]

            results = await asyncio.gather(*tasks)

            # æ›´æ–°ç»Ÿè®¡
            response_time = time.time() - start_time
            self.stats['total_requests'] += len(requests)
            self.stats['successful_requests'] += len([r for r in results if 'error' not in r])
            self.stats['failed_requests'] += len([r for r in results if 'error' in r])
            self._update_average_response_time(response_time)

            return web.json_response({
                'results': results,
                'count': len(results),
                'timestamp': datetime.now().isoformat()
            })

        except Exception as e:
            return web.json_response(
                {'error': str(e), 'timestamp': datetime.now().isoformat()},
                status=400
            )

    async def health_handler(self, request):
        """å¥åº·æ£€æŸ¥"""
        # ç³»ç»Ÿèµ„æºæ£€æŸ¥
        cpu_percent = psutil.cpu_percent()
        memory = psutil.virtual_memory()
        gpu_info = []

        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                gpu_memory = torch.cuda.get_device_properties(i).total_memory
                gpu_allocated = torch.cuda.memory_allocated(i)
                gpu_info.append({
                    'gpu_id': i,
                    'name': torch.cuda.get_device_name(i),
                    'total_memory': gpu_memory,
                    'allocated_memory': gpu_allocated,
                    'utilization': gpu_allocated / gpu_memory * 100
                })

        health_status = {
            'status': 'healthy',
            'timestamp': datetime.now().isoformat(),
            'uptime': time.time() - self.stats['start_time'],
            'system': {
                'cpu_percent': cpu_percent,
                'memory_percent': memory.percent,
                'memory_available': memory.available,
                'gpu_info': gpu_info
            },
            'model': {
                'version': self.inference_engine.model_manager.current_model_version,
                'device_count': self.inference_engine.model_manager.device_count
            }
        }

        return web.json_response(health_status)

    async def stats_handler(self, request):
        """ç»Ÿè®¡ä¿¡æ¯"""
        uptime = time.time() - self.stats['start_time']
        qps = self.stats['total_requests'] / uptime if uptime > 0 else 0

        stats = {
            **self.stats,
            'uptime': uptime,
            'qps': qps,
            'success_rate': (self.stats['successful_requests'] / max(self.stats['total_requests'], 1)) * 100
        }

        return web.json_response(stats)

    async def model_info_handler(self, request):
        """æ¨¡å‹ä¿¡æ¯"""
        info = {
            'model_architecture': 'IndustryLeadingTransformer',
            'model_version': self.inference_engine.model_manager.current_model_version,
            'device_count': self.inference_engine.model_manager.device_count,
            'batch_size': self.inference_engine.batch_size,
            'max_sequence_length': self.inference_engine.max_sequence_length,
            'capabilities': [
                'Multi-task prediction',
                'Direction forecasting',
                'Return estimation',
                'Volatility prediction',
                'Risk assessment',
                'Real-time inference'
            ]
        }

        return web.json_response(info)

    def _update_average_response_time(self, response_time: float):
        """æ›´æ–°å¹³å‡å“åº”æ—¶é—´"""
        if self.stats['successful_requests'] == 1:
            self.stats['average_response_time'] = response_time
        else:
            self.stats['average_response_time'] = (
                (self.stats['average_response_time'] * (self.stats['successful_requests'] - 1) + response_time) /
                self.stats['successful_requests']
            )

    def run(self, host: str = '0.0.0.0', port: int = 8001):
        """å¯åŠ¨æœåŠ¡å™¨"""
        print(f"ğŸš€ å¯åŠ¨å·¥ä¸šçº§APIæœåŠ¡å™¨")
        print(f"   åœ°å€: http://{host}:{port}")
        print(f"   æ‰¹å¤§å°: {self.inference_engine.batch_size}")

        # ä½¿ç”¨uvloopä¼˜åŒ–
        asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())

        web.run_app(self.app, host=host, port=port, access_log=None)

def main():
    """ä¸»éƒ¨ç½²å‡½æ•°"""
    print("ğŸš€ å·¥ä¸šçº§æ¨¡å‹éƒ¨ç½²ç³»ç»Ÿ")
    print("=" * 80)

    # é…ç½®
    model_config = get_model_config()
    deployment_config = {
        'model_path': 'models/best_industrial_model.pth',
        'batch_size': 32,
        'batch_timeout': 0.1,
        'max_sequence_length': 252,
        'inference_workers': 4,
        'use_amp': True
    }

    print("ğŸ“Š é…ç½®ä¿¡æ¯:")
    print(f"   æ¨¡å‹è·¯å¾„: {deployment_config['model_path']}")
    print(f"   æ‰¹å¤§å°: {deployment_config['batch_size']}")
    print(f"   æ¨ç†å·¥ä½œçº¿ç¨‹: {deployment_config['inference_workers']}")

    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not Path(deployment_config['model_path']).exists():
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {deployment_config['model_path']}")
        print("ğŸ’¡ è¯·å…ˆè®­ç»ƒæ¨¡å‹æˆ–æä¾›æ­£ç¡®çš„æ¨¡å‹è·¯å¾„")
        return

    try:
        # åˆ›å»ºæ¨¡å‹ç®¡ç†å™¨
        model_manager = ModelManager(
            model_path=deployment_config['model_path'],
            config=model_config
        )

        # åˆ›å»ºæ¨ç†å¼•æ“
        inference_engine = IndustrialInferenceEngine(
            model_manager=model_manager,
            config=deployment_config
        )

        # åˆ›å»ºAPIæœåŠ¡å™¨
        api_server = IndustrialAPIServer(inference_engine)

        # å¯åŠ¨æœåŠ¡
        api_server.run(host='0.0.0.0', port=8001)

    except KeyboardInterrupt:
        print("\nğŸ›‘ æœåŠ¡å™¨å…³é—­")
    except Exception as e:
        print(f"âŒ éƒ¨ç½²å¤±è´¥: {e}")

if __name__ == "__main__":
    main()